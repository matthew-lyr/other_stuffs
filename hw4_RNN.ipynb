{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthew-lyr/other_stuffs/blob/main/hw4_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r67y9UpchZ38"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "本次作業是要讓同學接觸 NLP 當中一個簡單的 task —— 語句分類（文本分類）\n",
        "\n",
        "給定一個語句，判斷他有沒有惡意（負面標 1，正面標 0）\n",
        "\n",
        "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ajS_WskRo0S"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# path_prefix = 'drive/My Drive/Colab Notebooks/hw4 - Recurrent Neural Network'\n",
        "path_prefix = './'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YrAlczfM_w6"
      },
      "source": [
        "### Download Dataset\n",
        "有三個檔案，分別是 training_label.txt、training_nolabel.txt、testing_data.txt\n",
        "\n",
        "- training_label.txt：有 label 的 training data（句子配上 0 or 1，+++$+++ 只是分隔符號，不要理它）\n",
        "    - e.g., 1 +++$+++ are wtf ... awww thanks !\n",
        "\n",
        "- training_nolabel.txt：沒有 label 的 training data（只有句子），用來做 semi-supervised learning\n",
        "    - ex: hates being this burnt !! ouch\n",
        "\n",
        "- testing_data.txt：你要判斷 testing data 裡面的句子是 0 or 1\n",
        "\n",
        "    >id,text\n",
        "\n",
        "    >0,my dog ate our dinner . no , seriously ... he ate it .\n",
        "\n",
        "    >1,omg last day sooon n of primary noooooo x im gona be swimming out of school wif the amount of tears am gona cry\n",
        "\n",
        "    >2,stupid boys .. they ' re so .. stupid !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2gwKORmuViJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a62a69fa-0356-4ea7-ac66-fc22f058a9e6"
      },
      "source": [
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1dPHIl8ZnfDz_fxNd2ZeBYedTat2lfxcO' -O 'drive/My Drive/Colab Notebooks/hw8-RNN/data/training_label.txt'\n",
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1x1rJOX_ETqnOZjdMAbEE2pqIjRNa8xcc' -O 'drive/My Drive/Colab Notebooks/hw8-RNN/data/training_nolabel.txt'\n",
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=16CtnQwSDCob9xmm6EdHHR7PNFNiOrQ30' -O 'drive/My Drive/Colab Notebooks/hw8-RNN/data/testing_data.txt'\n",
        "\n",
        "!gdown --id '1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8\n",
            "To: /content/data.zip\n",
            "45.1MB [00:00, 143MB/s] \n",
            "Archive:  data.zip\n",
            "  inflating: training_label.txt      \n",
            "  inflating: testing_data.txt        \n",
            "  inflating: training_nolabel.txt    \n",
            "data.zip     testing_data.txt\t training_nolabel.txt\n",
            "sample_data  training_label.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hDIokoP6464"
      },
      "source": [
        "# this is for filtering the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc143hSvNGr6"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICDIhhgCY2-M"
      },
      "source": [
        "# 什么是torch.eq\n",
        "# utils.py\n",
        "# 這個 block 用來先定義一些等等常用到的函式\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def load_training_data(path='training_label.txt'):\n",
        "    # 把 training 時需要的 data 讀進來\n",
        "    # 如果是 'training_label.txt'，需要讀取 label，如果是 'training_nolabel.txt'，不需要讀取 label\n",
        "    if 'training_label' in path:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
        "        x = [line[2:] for line in lines]\n",
        "        y = [line[0] for line in lines]\n",
        "        return x, y\n",
        "    else:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            x = [line.strip('\\n').split(' ') for line in lines]\n",
        "        return x\n",
        "\n",
        "def load_testing_data(path='testing_data'):\n",
        "    # 把 testing 時需要的 data 讀進來\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n",
        "        X = [sen.split(' ') for sen in X]\n",
        "    return X\n",
        "\n",
        "def evaluation(outputs, labels):\n",
        "    # outputs => probability (float)\n",
        "    # labels => labels\n",
        "    outputs[outputs>=0.5] = 1 # 大於等於 0.5 為正面\n",
        "    outputs[outputs<0.5] = 0 # 小於 0.5 為負面\n",
        "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
        "    return correct"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYE8UYQsNIxM"
      },
      "source": [
        "### Train Word to Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgGWaF8_2S3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba8ca33-1825-4fd5-9faa-4c5efdec718a"
      },
      "source": [
        "# w2v.py\n",
        "# 這個 block 是用來訓練 word to vector 的 word embedding\n",
        "# 注意！這個 block 在訓練 word to vector 時是用 cpu，可能要花到 10 分鐘以上\n",
        "# 什么是vector size，window，min_count(字出现少于这个次数就删了), workers, iter, sg（是skip gram还是bag of words）\n",
        "# skip gram是target word的one-hot vector去同时预测window内所有words的one-hot vectors\n",
        "# continuous bag of word是反过来的，一堆字预测一个target word\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from gensim.models import word2vec\n",
        "\n",
        "def train_word2vec(x):\n",
        "    # 訓練 word to vector 的 word embedding\n",
        "    model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"loading training data ...\")\n",
        "    train_x, y = load_training_data('training_label.txt')\n",
        "    train_x_no_label = load_training_data('training_nolabel.txt')\n",
        "\n",
        "    print(\"loading testing data ...\")\n",
        "    test_x = load_testing_data('testing_data.txt')\n",
        "\n",
        "    #model = train_word2vec(train_x + train_x_no_label + test_x)\n",
        "    model = train_word2vec(train_x + test_x)\n",
        "    \n",
        "    print(\"saving model ...\")\n",
        "    # model.save(os.path.join(path_prefix, 'model/w2v_all.model'))\n",
        "    model.save(os.path.join(path_prefix, 'w2v_all.model'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading training data ...\n",
            "loading testing data ...\n",
            "saving model ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl6UamYDK6RW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5617ff9-1050-4cde-b3a7-431014e21cab"
      },
      "source": [
        "model.vector_size"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wHLtS0wNR6w"
      },
      "source": [
        "### Data Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfGKiOitk5ob"
      },
      "source": [
        "# preprocess.py\n",
        "# no offense 这个block写的就是傻逼，写个class多此一举\n",
        "# 這個 block 用來做 data 的預處理\n",
        "from torch import nn\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "class Preprocess():\n",
        "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n",
        "        self.w2v_path = w2v_path\n",
        "        self.sentences = sentences\n",
        "        self.sen_len = sen_len\n",
        "        self.idx2word = []\n",
        "        self.word2idx = {}\n",
        "        self.embedding_matrix = []\n",
        "    def get_w2v_model(self):\n",
        "        # 把之前訓練好的 word to vec 模型讀進來\n",
        "        self.embedding = Word2Vec.load(self.w2v_path)\n",
        "        self.embedding_dim = self.embedding.vector_size\n",
        "    def add_embedding(self, word):\n",
        "        # 把 word 加進 embedding，並賦予他一個隨機生成的 representation vector\n",
        "        # what's torch.nn.init.uniform_？\n",
        "        # 是在initialize weights\n",
        "        # word 只會是 \"<PAD>\" 或 \"<UNK>\"\n",
        "        vector = torch.empty(1, self.embedding_dim)\n",
        "        torch.nn.init.uniform_(vector)\n",
        "        self.word2idx[word] = len(self.word2idx)\n",
        "        self.idx2word.append(word)\n",
        "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
        "    def make_embedding(self, load=True):\n",
        "        print(\"Get embedding ...\")\n",
        "        # 取得訓練好的 Word2vec word embedding\n",
        "        if load:\n",
        "            print(\"loading word to vec model ...\")\n",
        "            self.get_w2v_model()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # 製作一個 word2idx 的 dictionary\n",
        "        # 製作一個 idx2word 的 list\n",
        "        # 製作一個 word2vector 的 list\n",
        "        for i, word in enumerate(self.embedding.wv.vocab):\n",
        "            print('get words #{}'.format(i+1), end='\\r')\n",
        "            #e.g. self.word2index['he'] = 1 \n",
        "            #e.g. self.index2word[1] = 'he'\n",
        "            #e.g. self.vectors[1] = 'he' vector\n",
        "            self.word2idx[word] = len(self.word2idx)\n",
        "            self.idx2word.append(word)\n",
        "            self.embedding_matrix.append(self.embedding[word])\n",
        "        print('')\n",
        "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
        "        # 將 \"<PAD>\" 跟 \"<UNK>\" 加進 embedding 裡面\n",
        "        self.add_embedding(\"<PAD>\")\n",
        "        self.add_embedding(\"<UNK>\")\n",
        "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
        "        return self.embedding_matrix\n",
        "    def pad_sequence(self, sentence):\n",
        "        # 將每個句子變成一樣的長度\n",
        "        # 应该是pad sentence id因为这个不是句子，而是被转化成id的句子\n",
        "        if len(sentence) > self.sen_len:\n",
        "            sentence = sentence[:self.sen_len]\n",
        "        else:\n",
        "            pad_len = self.sen_len - len(sentence)\n",
        "            for _ in range(pad_len):\n",
        "                sentence.append(self.word2idx[\"<PAD>\"])\n",
        "        assert len(sentence) == self.sen_len\n",
        "        return sentence\n",
        "    def sentence_word2idx(self):\n",
        "        # 把句子裡面的字轉成相對應的 index\n",
        "        sentence_list = []\n",
        "        for i, sen in enumerate(self.sentences):\n",
        "            print('sentence count #{}'.format(i+1), end='\\r')\n",
        "            sentence_idx = []\n",
        "            for word in sen:\n",
        "                if (word in self.word2idx.keys()):\n",
        "                    sentence_idx.append(self.word2idx[word])\n",
        "                else:\n",
        "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
        "            # 將每個句子變成一樣的長度\n",
        "            sentence_idx = self.pad_sequence(sentence_idx)\n",
        "            sentence_list.append(sentence_idx)\n",
        "        return torch.LongTensor(sentence_list)\n",
        "    def labels_to_tensor(self, y):\n",
        "        # 把 labels 轉成 tensor\n",
        "        y = [int(label) for label in y]\n",
        "        return torch.LongTensor(y)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC_dbtTvepvj"
      },
      "source": [
        "# words = []\n",
        "# for i, word in enumerate(model.wv.vocab):\n",
        "#     words.append(word)\n",
        "# words  \n",
        "# model['ate'].shape\n",
        "\n",
        "embedding = []\n",
        "for i, word in enumerate(model.wv.vocab):\n",
        "    embedding.append(model[word])\n",
        "embedding = torch.tensor(embedding)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQF2zyuLg5yd",
        "outputId": "2d075d15-a476-46a8-f790-2cc6c300e109",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "embedding"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0881,  0.2244, -0.0161,  ...,  0.0138, -0.2250,  0.1842],\n",
              "        [ 0.0135, -0.0275, -0.2926,  ...,  0.0954, -0.0447, -0.2557],\n",
              "        [ 0.0637,  0.1814,  0.0685,  ..., -0.0070, -0.1157,  0.0535],\n",
              "        ...,\n",
              "        [-0.0078, -0.0156, -0.0111,  ...,  0.1844, -0.3107, -0.0750],\n",
              "        [-0.0678,  0.0389,  0.1543,  ...,  0.0513, -0.1473,  0.1277],\n",
              "        [ 0.0294,  0.1446,  0.1994,  ...,  0.0735, -0.2691,  0.1257]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WJB7go5NWL0"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XketwKs4lFfB"
      },
      "source": [
        "# data.py\n",
        "# 實作了 dataset 所需要的 '__init__', '__getitem__', '__len__'\n",
        "# 好讓 dataloader 能使用\n",
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class TwitterDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Expected data shape like:(data_num, data_len)\n",
        "    Data can be a list of numpy array or a list of lists\n",
        "    input data shape : (data_num, seq_len, feature_dim)\n",
        "    \n",
        "    __len__ will return the number of data\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.label = y\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is None: return self.data[idx]\n",
        "        return self.data[idx], self.label[idx]\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNJ8xWIMNa2r"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v67syGgDhOEq"
      },
      "source": [
        "# embedding\n",
        "embedding = []\n",
        "for i, word in enumerate(model.wv.vocab):\n",
        "    embedding.append(model[word])\n",
        "embedding = torch.tensor(embedding)\n",
        "# embedding layer\n",
        "embedding_layer = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
        "embedding_layer.weight = torch.nn.Parameter(embedding)\n",
        "\n",
        "\n",
        "embedding.weight.requires_grad = False if fix_embedding else True\n",
        "embedding_dim = embedding.size(1)\n",
        "hidden_dim = hidden_dim\n",
        "num_layers = num_layers\n",
        "dropout = dropout\n",
        "lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU5mdAsjheHK"
      },
      "source": [
        "# print(embedding[0])\n",
        "# print(embedding_layer(torch.LongTensor([0])))\n",
        "train_x, y = load_training_data('training_label.txt')\n",
        "# train_x\n",
        "# input = embedding_layer(train_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS6RJADulIq1"
      },
      "source": [
        "# model.py\n",
        "# 這個 block 是要拿來訓練的模型\n",
        "# embedding和embedding layer有什么区别\n",
        "# embedding layer 吃什么？\n",
        "# LSTM里面是怎么回事，吃什么，吐什么\n",
        "# LSTM吃inputs，但what the fuck is in inputs\n",
        "# inputs是被embedding layer处理过的sentence_word2idx（）的结果\n",
        "# sentence_word2idx处理的结果长什么样\n",
        "# 从txt files直接读出来的是arrays of arrays 最里面的每个array是个句子，里面的element是文字版的单词，然后最外层的array就是所有句子的集合\n",
        "# embedding layer在干什么\n",
        "# \n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "class LSTM_Net(nn.Module):\n",
        "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
        "        super(LSTM_Net, self).__init__()\n",
        "        # 製作 embedding layer\n",
        "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
        "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
        "        # 是否將 embedding fix 住，如果 fix_embedding 為 False，在訓練過程中，embedding 也會跟著被訓練\n",
        "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
        "        self.embedding_dim = embedding.size(1)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
        "                                         nn.Linear(hidden_dim, 1),\n",
        "                                         nn.Sigmoid() )\n",
        "    def forward(self, inputs):\n",
        "        inputs = self.embedding(inputs)\n",
        "        x, _ = self.lstm(inputs, None)\n",
        "        # x 的 dimension (batch, seq_len, hidden_size)\n",
        "        # 取用 LSTM 最後一層的 hidden state\n",
        "        x = x[:, -1, :] \n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9UeRrwXd19M",
        "outputId": "18692789-b23b-408a-b0f3-37c836385df9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# main.py\n",
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from gensim.models import word2vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 處理好各個 data 的路徑\n",
        "train_with_label = os.path.join(path_prefix, 'training_label.txt')\n",
        "\n",
        "w2v_path = os.path.join(path_prefix, 'w2v_all.model') # 處理 word to vec model 的路徑\n",
        "\n",
        "train_x, y = load_training_data(train_with_label)\n",
        "\n",
        "print(train_x[:10])\n",
        "print(y[:10])\n",
        "\n",
        "# # 對 input 跟 labels 做預處理\n",
        "embedding = []\n",
        "for i, word in enumerate(model.wv.vocab):\n",
        "    embedding.append(model[word])\n",
        "embedding = torch.tensor(embedding)\n",
        "\n",
        "print(embedding[:10])\n",
        "\n",
        "word2idx = {}\n",
        "\n",
        "for i, word in enumerate(model.wv.vocab):\n",
        "    word2idx[word] = len(word2idx)\n",
        "\n",
        "word2idx['<PAD>'] = len(word2idx)\n",
        "vector = torch.empty(1, embedding.shape[1])\n",
        "torch.nn.init.uniform_(vector)\n",
        "embedding = torch.cat([embedding, vector], 0)\n",
        "\n",
        "word2idx['<UNK>'] = len(word2idx)\n",
        "vector = torch.empty(1, embedding.shape[1])\n",
        "torch.nn.init.uniform_(vector)\n",
        "embedding = torch.cat([embedding, vector], 0)\n",
        "\n",
        "sentence_list = []\n",
        "for i, sen in enumerate(train_x):\n",
        "    sentence_idx = []\n",
        "    for word in sen:\n",
        "        if (word in word2idx.keys()):\n",
        "            sentence_idx.append(word2idx[word])\n",
        "        else:\n",
        "            sentence_idx.append(word2idx[\"<UNK>\"])\n",
        "            # 將每個句子變成一樣的長度\n",
        "    if len(sentence_idx) > 7:\n",
        "        sentence_idx = sentence_idx[:7]\n",
        "    else:\n",
        "        pad_len = 7 - len(sentence_idx)\n",
        "        for _ in range(pad_len):\n",
        "            sentence_idx.append(word2idx[\"<PAD>\"])\n",
        "    sentence_list.append(sentence_idx)\n",
        "# sentence_word2idx的结果\n",
        "train_x = torch.LongTensor(sentence_list)\n",
        "\n",
        "print(train_x[:10])\n",
        "\n",
        "y = [int(label) for label in y]\n",
        "y = torch.LongTensor(y)\n",
        "\n",
        "print(y[:10])\n",
        "\n",
        "\n",
        "\n",
        "# embedding layer\n",
        "embedding_layer = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
        "embedding_layer.weight = torch.nn.Parameter(embedding)\n",
        "\n",
        "\n",
        "embedding_layer.weight.requires_grad = False \n",
        "embedding_dim = embedding.size(1)\n",
        "\n",
        "print(embedding_dim)\n",
        "print(embedding.shape)\n",
        "\n",
        "hidden_dim = 100\n",
        "num_layers = 10\n",
        "dropout = 0.5\n",
        "\n",
        "lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "\n",
        "\n",
        "# # 製作一個 model 的對象\n",
        "# model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
        "# training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['are', 'wtf', '...', 'awww', 'thanks', '!'], ['leavingg', 'to', 'wait', 'for', 'kaysie', 'to', 'arrive', 'myspacin', 'itt', 'for', 'now', 'ilmmthek', '.!'], ['i', 'wish', 'i', 'could', 'go', 'and', 'see', 'duffy', 'when', 'she', 'comes', 'to', 'mamaia', 'romania', '.'], ['i', 'know', 'eep', '!', 'i', 'can', \"'\", 't', 'wait', 'for', 'one', 'more', 'day', '....'], ['so', 'scared', 'and', 'feeling', 'sick', '.', 'fuck', '!', 'hope', 'someone', 'at', 'hr', 'help', '...', 'wish', 'it', 'would', 'be', 'wendita', 'or', 'karen', '.'], ['my', 'b', 'day', 'was', 'thurs', '.', 'i', 'wanted', '2', 'do', '5', 'this', 'weekend', 'for', 'my', 'b', 'day', 'but', 'i', 'guess', 'close', 'enough', 'next', 'weekend', '.', 'going', 'alone'], ['e3', 'is', 'in', 'the', 'trending', 'topics', 'only', 'just', 'noticed', 'ive', 'been', 'tweeting', 'on', 'my', 'iphone', 'until', 'now'], ['where', 'did', 'you', 'get', 'him', 'from', 'i', 'know', 'someone', 'who', 'would', 'love', 'that', '!'], ['dam', 'just', 'got', 'buzzed', 'by', 'another', 'huge', 'fly', '!', 'this', 'time', 'it', 'landed', 'on', 'my', 'head', '...', 'not', 'impressed'], ['tomorrowwwwwwwww', '!!!', 'you', \"'\", 'll', 'love', 'tomorrow', \"'\", 's', 'news', '!']]\n",
            "['1', '1', '0', '1', '0', '0', '1', '1', '0', '1']\n",
            "tensor([[-0.2344, -0.1906,  0.0504,  ..., -0.1972,  0.0304, -0.1410],\n",
            "        [ 0.1448,  0.0139, -0.2921,  ..., -0.2492, -0.1936,  0.0598],\n",
            "        [-0.1230, -0.0452,  0.1611,  ..., -0.2479, -0.1225,  0.0611],\n",
            "        ...,\n",
            "        [-0.0298, -0.3123,  0.0188,  ...,  0.1792, -0.1787,  0.2682],\n",
            "        [ 0.1622, -0.3672,  0.0839,  ..., -0.0866,  0.1954,  0.0680],\n",
            "        [-0.1655, -0.3279,  0.0510,  ...,  0.2309, -0.2108, -0.3705]])\n",
            "tensor([[    0,     1,     2,     3,     4,     5, 24694],\n",
            "        [24695,     6,     7,     8, 24695,     6,     9],\n",
            "        [   13,    14,    13,    15,    16,    17,    18],\n",
            "        [   13,    24,    25,     5,    13,    26,    27],\n",
            "        [   33,    34,    17,    35,    36,    23,    37],\n",
            "        [   48,    49,    31,    50,    51,    23,    13],\n",
            "        [   65,    66,    67,    68,    69,    70,    71],\n",
            "        [   80,    81,    82,    83,    84,    85,    13],\n",
            "        [   89,    72,    90,    91,    92,    93,    94],\n",
            "        [24695,   101,    82,    27,   102,    87,   103]])\n",
            "tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1])\n",
            "250\n",
            "torch.Size([24696, 250])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrlb4Qk3i1lj",
        "outputId": "7e165f92-e165-4d94-ecd0-6098f38c10fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(train_x.shape)\n",
        "# print(embedding_layer)\n",
        "input = embedding_layer(train_x)\n",
        "print(input.shape)\n",
        "lstm = nn.LSTM(input_size = 250, hidden_size = 8, num_layers = 5, batch_first=True)\n",
        "a = lstm(input)\n",
        "print(a[0].shape)\n",
        "\n",
        "print(a[0][0])\n",
        "print(a[1][0])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([200000, 7])\n",
            "torch.Size([200000, 7, 250])\n",
            "torch.Size([200000, 7, 8])\n",
            "tensor([[-0.0749, -0.1192, -0.0484, -0.0348, -0.0492, -0.0886,  0.0368, -0.1091],\n",
            "        [-0.1203, -0.1813, -0.0917, -0.0574, -0.0394, -0.1309,  0.0541, -0.1398],\n",
            "        [-0.1418, -0.2111, -0.1200, -0.0689, -0.0165, -0.1477,  0.0635, -0.1450],\n",
            "        [-0.1506, -0.2244, -0.1360, -0.0736,  0.0037, -0.1533,  0.0688, -0.1445],\n",
            "        [-0.1537, -0.2298, -0.1441, -0.0752,  0.0180, -0.1546,  0.0714, -0.1439],\n",
            "        [-0.1546, -0.2317, -0.1477, -0.0754,  0.0271, -0.1549,  0.0725, -0.1439],\n",
            "        [-0.1548, -0.2322, -0.1491, -0.0752,  0.0324, -0.1549,  0.0728, -0.1445]],\n",
            "       grad_fn=<SelectBackward>)\n",
            "tensor([[[ 5.4737e-01,  5.1302e-02, -3.3701e-01,  ...,  4.9958e-02,\n",
            "           6.0848e-02,  2.3822e-02],\n",
            "         [ 3.9914e-01, -1.2812e-01, -1.5358e-03,  ...,  2.2767e-01,\n",
            "           5.8594e-04,  1.0839e-01],\n",
            "         [-2.4257e-02, -2.7142e-01, -4.9810e-02,  ...,  1.7843e-01,\n",
            "          -3.0126e-01,  1.1906e-01],\n",
            "         ...,\n",
            "         [-2.6742e-01, -2.1291e-01,  1.1856e-01,  ...,  4.4746e-02,\n",
            "          -3.7768e-01,  1.2361e-01],\n",
            "         [-2.4803e-01, -3.6108e-01, -1.6114e-01,  ...,  1.1044e-01,\n",
            "          -9.1555e-02,  3.2840e-02],\n",
            "         [ 3.7130e-02, -2.3369e-01, -7.7120e-02,  ...,  4.4126e-02,\n",
            "          -3.9051e-01,  1.2278e-01]],\n",
            "\n",
            "        [[ 1.2225e-01,  4.3533e-02,  7.1999e-02,  ..., -1.3545e-01,\n",
            "          -7.2651e-02,  7.9693e-02],\n",
            "         [ 1.3268e-01,  5.7132e-02,  1.2194e-01,  ..., -1.2303e-01,\n",
            "          -1.2292e-01,  8.8987e-02],\n",
            "         [ 1.4073e-01,  7.4634e-02,  1.5494e-01,  ..., -1.0918e-01,\n",
            "           7.6073e-03,  3.7137e-02],\n",
            "         ...,\n",
            "         [ 1.4767e-01,  7.6148e-02,  1.5140e-01,  ..., -9.0010e-02,\n",
            "          -5.4410e-04,  6.8855e-02],\n",
            "         [ 2.0118e-01,  7.7763e-02,  7.0105e-02,  ..., -4.7021e-02,\n",
            "          -3.0465e-03,  9.1710e-02],\n",
            "         [ 1.8080e-01,  8.3453e-02,  1.7368e-01,  ..., -1.3351e-01,\n",
            "          -6.2044e-02,  5.4944e-02]],\n",
            "\n",
            "        [[-3.5167e-01,  9.7483e-02,  1.1432e-01,  ...,  6.1165e-02,\n",
            "           1.1886e-01,  1.0118e-01],\n",
            "         [-3.4924e-01,  9.8672e-02,  1.2437e-01,  ...,  6.2965e-02,\n",
            "           1.1719e-01,  1.0165e-01],\n",
            "         [-3.5566e-01,  1.1253e-01,  1.1685e-01,  ...,  6.5267e-02,\n",
            "           1.3020e-01,  9.4866e-02],\n",
            "         ...,\n",
            "         [-3.5526e-01,  1.1455e-01,  1.1914e-01,  ...,  6.0440e-02,\n",
            "           1.1694e-01,  9.6750e-02],\n",
            "         [-3.6076e-01,  1.1076e-01,  1.1455e-01,  ...,  5.4899e-02,\n",
            "           9.9573e-02,  1.0138e-01],\n",
            "         [-3.5965e-01,  9.0886e-02,  1.1467e-01,  ...,  6.7037e-02,\n",
            "           1.3784e-01,  9.6835e-02]],\n",
            "\n",
            "        [[-1.3131e-01, -1.8085e-01,  1.4378e-01,  ..., -1.6558e-01,\n",
            "           2.3770e-01,  9.4794e-03],\n",
            "         [-1.3035e-01, -1.8006e-01,  1.4192e-01,  ..., -1.6478e-01,\n",
            "           2.3717e-01,  8.1962e-03],\n",
            "         [-1.2967e-01, -1.7534e-01,  1.4147e-01,  ..., -1.6780e-01,\n",
            "           2.4163e-01,  7.8530e-03],\n",
            "         ...,\n",
            "         [-1.2859e-01, -1.7790e-01,  1.3860e-01,  ..., -1.6585e-01,\n",
            "           2.4014e-01,  7.5269e-03],\n",
            "         [-1.2735e-01, -1.8063e-01,  1.3835e-01,  ..., -1.6735e-01,\n",
            "           2.3952e-01,  7.7921e-03],\n",
            "         [-1.3166e-01, -1.7691e-01,  1.4496e-01,  ..., -1.6738e-01,\n",
            "           2.4041e-01,  8.8351e-03]],\n",
            "\n",
            "        [[-1.5485e-01, -2.3218e-01, -1.4911e-01,  ..., -1.5490e-01,\n",
            "           7.2845e-02, -1.4448e-01],\n",
            "         [-1.5478e-01, -2.3260e-01, -1.4924e-01,  ..., -1.5438e-01,\n",
            "           7.2612e-02, -1.4479e-01],\n",
            "         [-1.5448e-01, -2.3279e-01, -1.4879e-01,  ..., -1.5527e-01,\n",
            "           7.2835e-02, -1.4405e-01],\n",
            "         ...,\n",
            "         [-1.5458e-01, -2.3316e-01, -1.4921e-01,  ..., -1.5432e-01,\n",
            "           7.2404e-02, -1.4489e-01],\n",
            "         [-1.5469e-01, -2.3253e-01, -1.4898e-01,  ..., -1.5431e-01,\n",
            "           7.2122e-02, -1.4541e-01],\n",
            "         [-1.5446e-01, -2.3242e-01, -1.4875e-01,  ..., -1.5544e-01,\n",
            "           7.2984e-02, -1.4386e-01]]], grad_fn=<StackBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzb6M2CJ5KZh",
        "outputId": "3c51ebd6-2b67-4f96-b685-f006aee28e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        }
      },
      "source": [
        "from torch import nn\n",
        "#  5 \n",
        "x = torch.rand([5,4,3])\n",
        "print(x)\n",
        "x = nn.LSTM(, num_layers=2, batch_first=True)(x,None)\n",
        "print(x)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.3569, 0.6196, 0.0428],\n",
            "         [0.2071, 0.3757, 0.6306],\n",
            "         [0.2115, 0.1512, 0.8808],\n",
            "         [0.8912, 0.1054, 0.2327]],\n",
            "\n",
            "        [[0.9929, 0.6118, 0.0184],\n",
            "         [0.4313, 0.3615, 0.8026],\n",
            "         [0.4923, 0.0164, 0.8610],\n",
            "         [0.5669, 0.3129, 0.5037]],\n",
            "\n",
            "        [[0.8783, 0.2691, 0.9538],\n",
            "         [0.9745, 0.1745, 0.1450],\n",
            "         [0.0115, 0.6145, 0.7512],\n",
            "         [0.4000, 0.8153, 0.1036]],\n",
            "\n",
            "        [[0.3495, 0.1458, 0.3919],\n",
            "         [0.5251, 0.3193, 0.9116],\n",
            "         [0.7709, 0.2423, 0.6467],\n",
            "         [0.9586, 0.7667, 0.3692]],\n",
            "\n",
            "        [[0.1175, 0.8370, 0.9423],\n",
            "         [0.8454, 0.3778, 0.0580],\n",
            "         [0.9952, 0.6535, 0.5155],\n",
            "         [0.2686, 0.0030, 0.5438]]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4e59600a27b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_cell_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, proj_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproj_size\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"proj_size should be a positive integer or zero to disable projections\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mproj_size\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"proj_size has to be smaller than hidden_size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'int' and 'tuple'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWlpEL0sNc10"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QR4MMz-lR7i"
      },
      "source": [
        "# train.py\n",
        "# 這個 block 是用來訓練模型的\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
        "    model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數\n",
        "    criterion = nn.BCELoss() # 定義損失函數，這裡我們使用 binary cross entropy loss\n",
        "    t_batch = len(train) \n",
        "    v_batch = len(valid) \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr) # 將模型的參數給 optimizer，並給予適當的 learning rate\n",
        "    total_loss, total_acc, best_acc = 0, 0, 0\n",
        "    for epoch in range(n_epoch):\n",
        "        total_loss, total_acc = 0, 0\n",
        "        # 這段做 training\n",
        "        for i, (inputs, labels) in enumerate(train):\n",
        "            inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
        "            labels = labels.to(device, dtype=torch.float) # device為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
        "            optimizer.zero_grad() # 由於 loss.backward() 的 gradient 會累加，所以每次餵完一個 batch 後需要歸零\n",
        "            outputs = model(inputs) # 將 input 餵給模型\n",
        "            outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n",
        "            loss = criterion(outputs, labels) # 計算此時模型的 training loss\n",
        "            loss.backward() # 算 loss 的 gradient\n",
        "            optimizer.step() # 更新訓練模型的參數\n",
        "            correct = evaluation(outputs, labels) # 計算此時模型的 training accuracy\n",
        "            total_acc += (correct / batch_size)\n",
        "            total_loss += loss.item()\n",
        "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
        "            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n",
        "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
        "\n",
        "        # 這段做 validation\n",
        "        model.eval() # 將 model 的模式設為 eval，這樣 model 的參數就會固定住\n",
        "        with torch.no_grad():\n",
        "            total_loss, total_acc = 0, 0\n",
        "            for i, (inputs, labels) in enumerate(valid):\n",
        "                inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
        "                labels = labels.to(device, dtype=torch.float) # device 為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
        "                outputs = model(inputs) # 將 input 餵給模型\n",
        "                outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n",
        "                loss = criterion(outputs, labels) # 計算此時模型的 validation loss\n",
        "                correct = evaluation(outputs, labels) # 計算此時模型的 validation accuracy\n",
        "                total_acc += (correct / batch_size)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
        "            if total_acc > best_acc:\n",
        "                # 如果 validation 的結果優於之前所有的結果，就把當下的模型存下來以備之後做預測時使用\n",
        "                best_acc = total_acc\n",
        "                #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n",
        "                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n",
        "                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
        "        print('-----------------------------------------------')\n",
        "        model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數（因為剛剛轉成 eval 模式）"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF5YQrupNfCS"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X2wkdAYxHYA"
      },
      "source": [
        "# test.py\n",
        "# 這個 block 用來對 testing_data.txt 做預測\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def testing(batch_size, test_loader, model, device):\n",
        "    model.eval()\n",
        "    ret_output = []\n",
        "    with torch.no_grad():\n",
        "        for i, inputs in enumerate(test_loader):\n",
        "            inputs = inputs.to(device, dtype=torch.long)\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.squeeze()\n",
        "            outputs[outputs>=0.5] = 1 # 大於等於 0.5 為正面\n",
        "            outputs[outputs<0.5] = 0 # 小於 0.5 為負面\n",
        "            ret_output += outputs.int().tolist()\n",
        "    \n",
        "    return ret_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfnKj0KXNeoz"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EztIWqCmlZof"
      },
      "source": [
        "# main.py\n",
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from gensim.models import word2vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 通過 torch.cuda.is_available() 的回傳值進行判斷是否有使用 GPU 的環境，如果有的話 device 就設為 \"cuda\"，沒有的話就設為 \"cpu\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 處理好各個 data 的路徑\n",
        "train_with_label = os.path.join(path_prefix, 'training_label.txt')\n",
        "train_no_label = os.path.join(path_prefix, 'training_nolabel.txt')\n",
        "testing_data = os.path.join(path_prefix, 'testing_data.txt')\n",
        "\n",
        "w2v_path = os.path.join(path_prefix, 'w2v_all.model') # 處理 word to vec model 的路徑\n",
        "\n",
        "# 定義句子長度、要不要固定 embedding、batch 大小、要訓練幾個 epoch、learning rate 的值、model 的資料夾路徑\n",
        "sen_len = 20\n",
        "fix_embedding = True # fix embedding during training\n",
        "batch_size = 128\n",
        "epoch = 5\n",
        "lr = 0.001\n",
        "# model_dir = os.path.join(path_prefix, 'model/') # model directory for checkpoint model\n",
        "model_dir = path_prefix # model directory for checkpoint model\n",
        "\n",
        "print(\"loading data ...\") # 把 'training_label.txt' 跟 'training_nolabel.txt' 讀進來\n",
        "train_x, y = load_training_data(train_with_label)\n",
        "train_x_no_label = load_training_data(train_no_label)\n",
        "\n",
        "# 對 input 跟 labels 做預處理\n",
        "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "train_x = preprocess.sentence_word2idx()\n",
        "y = preprocess.labels_to_tensor(y)\n",
        "\n",
        "# 製作一個 model 的對象\n",
        "model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
        "model = model.to(device) # device為 \"cuda\"，model 使用 GPU 來訓練（餵進去的 inputs 也需要是 cuda tensor）\n",
        "\n",
        "# 把 data 分為 training data 跟 validation data（將一部份 training data 拿去當作 validation data）\n",
        "X_train, X_val, y_train, y_val = train_x[:180000], train_x[180000:], y[:180000], y[180000:]\n",
        "\n",
        "# 把 data 做成 dataset 供 dataloader 取用\n",
        "train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
        "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
        "\n",
        "# 把 data 轉成 batch of tensors\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = True,\n",
        "                                            num_workers = 8)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = False,\n",
        "                                            num_workers = 8)\n",
        "\n",
        "# 開始訓練\n",
        "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fQeaQNeNm3L"
      },
      "source": [
        "### Predict and Write to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFvjFQopxVrt"
      },
      "source": [
        "# 開始測試模型並做預測\n",
        "print(\"loading testing data ...\")\n",
        "test_x = load_testing_data(testing_data)\n",
        "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "test_x = preprocess.sentence_word2idx()\n",
        "test_dataset = TwitterDataset(X=test_x, y=None)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = False,\n",
        "                                            num_workers = 8)\n",
        "print('\\nload model ...')\n",
        "model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
        "outputs = testing(batch_size, test_loader, model, device)\n",
        "\n",
        "# 寫到 csv 檔案供上傳 Kaggle\n",
        "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n",
        "print(\"save csv ...\")\n",
        "tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n",
        "print(\"Finish Predicting\")\n",
        "\n",
        "# 以下是使用 command line 上傳到 Kaggle 的方式\n",
        "# 需要先 pip install kaggle、Create API Token，詳細請看 https://github.com/Kaggle/kaggle-api 以及 https://www.kaggle.com/code1110/how-to-submit-from-google-colab\n",
        "# kaggle competitions submit [competition-name] -f [csv file path]] -m [message]\n",
        "# e.g., kaggle competitions submit ml-2020spring-hw4 -f output/predict.csv -m \"......\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSvgTRuGu2Qb"
      },
      "source": [
        "#### Check where the files are"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SZYJQ62utiK"
      },
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZf3E2O1wsQo"
      },
      "source": [
        "#### Download the files to your computer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzsAmmRUwqdA"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('predict.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}