{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "PyTorch_Introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zqyBFdobMTha",
        "jlzoXa6UMThg",
        "l08nQdE9MThp",
        "Tmg4eFQAMThr",
        "jifMOIcNMTh5",
        "aSO1McZLMTiT",
        "OCwLf9C2MTiY",
        "IrapEC2XMTiY",
        "NwsmNTYLMTig",
        "OyN-mHRoMTii"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthew-lyr/other_stuffs/blob/main/PyTorch_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Lg2NXGMThA"
      },
      "source": [
        "# PyTorch Introduction\n",
        "\n",
        "### TA: Chi-Liang Liu\n",
        "##### This Tutorial is modified from [University of Washington CSE446](https://courses.cs.washington.edu/courses/cse446/19au/section9.html) and [PyTorch Official Tutorials](https://pytorch.org/tutorials/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi_QP1bmMThC"
      },
      "source": [
        "Today, we will be intoducing PyTorch, \"an open source deep learning platform that provides a seamless path from research prototyping to production deployment\".\n",
        "\n",
        "This notebook is by no means comprehensive. If you have any questions the **documentation** and **Google** are your friends.\n",
        "\n",
        "Goal takeaways:\n",
        "- Automatic differentiation is a powerful tool\n",
        "- PyTorch implements common functions used in deep learning\n",
        "- Data Processing with PyTorch DataSet\n",
        "- Mixed Presision Training in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQIPkkKdMThD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(446)\n",
        "np.random.seed(446)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCQTBsnWMThH"
      },
      "source": [
        "## Tensors and relation to numpy\n",
        "\n",
        "By this point, we have worked with numpy quite a bit. PyTorch's basic building block, the `tensor` is similar to numpy's `ndarray`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXnFLFr1MThI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750fb6af-d71c-4f40-bfa9-d4797044fb4c"
      },
      "source": [
        "# torch.tensor 用来产生array和matrix\n",
        "# 不是太懂这里面matrix的dimension怎么量\n",
        "# 当用torch.sum(x,dim = )时候，dim是几，就消灭x的shape的哪一项\n",
        "# https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be\n",
        "\n",
        "# we create tensors in a similar way to numpy nd arrays\n",
        "x_numpy = np.array([0.1, 0.2, 0.3])\n",
        "x_torch = torch.tensor([0.1, 0.2, 0.3])\n",
        "print('x_numpy, x_torch')\n",
        "print(x_numpy, x_torch)\n",
        "print()\n",
        "\n",
        "# to and from numpy, pytorch\n",
        "print('to and from numpy and pytorch')\n",
        "print(torch.from_numpy(x_numpy), x_torch.numpy())\n",
        "print()\n",
        "\n",
        "# we can do basic operations like +-*/\n",
        "y_numpy = np.array([3,4,5.])\n",
        "y_torch = torch.tensor([3,4,5.])\n",
        "print(\"x+y\")\n",
        "print(x_numpy + y_numpy, x_torch + y_torch)\n",
        "print()\n",
        "\n",
        "# many functions that are in numpy are also in pytorch\n",
        "print(\"norm\")\n",
        "print(np.linalg.norm(x_numpy), torch.norm(x_torch))\n",
        "print()\n",
        "\n",
        "# to apply an operation along a dimension,\n",
        "# we use the dim keyword argument instead of axis\n",
        "print(\"mean along the 0th dimension\")\n",
        "x_numpy = np.array([[1,2],[3,4.]])\n",
        "x_torch = torch.tensor([[[1,2,1],[3,4,1]],[[5,6,1],[7,8,1]]])\n",
        "print(x_torch.shape)\n",
        "print(np.mean(x_numpy, axis=0), torch.max(x_torch, dim=1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_numpy, x_torch\n",
            "[0.1 0.2 0.3] tensor([0.1000, 0.2000, 0.3000])\n",
            "\n",
            "to and from numpy and pytorch\n",
            "tensor([0.1000, 0.2000, 0.3000], dtype=torch.float64) [0.1 0.2 0.3]\n",
            "\n",
            "x+y\n",
            "[3.1 4.2 5.3] tensor([3.1000, 4.2000, 5.3000])\n",
            "\n",
            "norm\n",
            "0.37416573867739417 tensor(0.3742)\n",
            "\n",
            "mean along the 0th dimension\n",
            "torch.Size([2, 2, 3])\n",
            "[2. 3.] torch.return_types.max(\n",
            "values=tensor([[3, 4, 1],\n",
            "        [7, 8, 1]]),\n",
            "indices=tensor([[1, 1, 0],\n",
            "        [1, 1, 0]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtyttsoZMThL"
      },
      "source": [
        "### `Tensor.view`\n",
        "We can use the `Tensor.view()` function to reshape tensors similarly to `numpy.reshape()`\n",
        "\n",
        "It can also automatically calculate the correct dimension if a `-1` is passed in. This is useful if we are working with batches, but the batch size is unknown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABhZ5mKpMThM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a25ddb99-7093-4603-e61b-8cd40ed66af6"
      },
      "source": [
        "# 不懂reshape完，element怎么排列，有什么规律\n",
        "# 感觉就是先弄成一行，一个dimension的array，然后再依次塞到新的matrix里\n",
        "# \"MNIST\"\n",
        "N, C, W, H = 10000, 3, 28, 28\n",
        "X = torch.randn((N, C, W, H))\n",
        "\n",
        "print(X.shape)\n",
        "print(X.view(N, C, 784).shape)\n",
        "print(X.view(-1, C, 784).shape) # automatically choose the 0th dimension"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10000, 3, 28, 28])\n",
            "torch.Size([10000, 3, 784])\n",
            "torch.Size([10000, 3, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiO8W1E8gl6z",
        "outputId": "2ca703ba-041c-4697-d256-92cc7c68c27a"
      },
      "source": [
        "x_torch = torch.tensor([[[1,2,1],[3,4,1]],[[5,6,1],[7,8,1]]])\n",
        "print(x_torch.shape)\n",
        "x_torch.view(-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2, 3])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 1, 3, 4, 1, 5, 6, 1, 7, 8, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXgxfCTMOjIp"
      },
      "source": [
        "### `BROADCASTING SEMANTICS`\n",
        "Two tensors are “broadcastable” if the following rules hold:\n",
        "\n",
        "Each tensor has at least one dimension.\n",
        "\n",
        "When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ioj-DAhOjiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f167cc23-edb0-4975-a011-39b8909b88c9"
      },
      "source": [
        "# PyTorch operations support NumPy Broadcasting Semantics.\n",
        "x=torch.rand(5,1,4,1)\n",
        "y=torch.empty(  3,1,1)\n",
        "print((x+y).size())\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3, 4, 1])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.3959],\n",
              "          [0.6177],\n",
              "          [0.7256],\n",
              "          [0.0971]]],\n",
              "\n",
              "\n",
              "        [[[0.9186],\n",
              "          [0.8277],\n",
              "          [0.4409],\n",
              "          [0.9344]]],\n",
              "\n",
              "\n",
              "        [[[0.8967],\n",
              "          [0.1897],\n",
              "          [0.6028],\n",
              "          [0.4290]]],\n",
              "\n",
              "\n",
              "        [[[0.8083],\n",
              "          [0.6786],\n",
              "          [0.3323],\n",
              "          [0.2304]]],\n",
              "\n",
              "\n",
              "        [[[0.0098],\n",
              "          [0.3594],\n",
              "          [0.4761],\n",
              "          [0.8707]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vQ3yD9MThP"
      },
      "source": [
        "## Computation graphs\n",
        "\n",
        "What's special about PyTorch's `tensor` object is that it implicitly creates a computation graph in the background. A computation graph is a a way of writing a mathematical expression as a graph. There is an algorithm to compute the gradients of all the variables of a computation graph in time on the same order it is to compute the function itself.\n",
        "\n",
        "Consider the expression $e=(a+b)*(b+1)$ with values $a=2, b=1$. We can draw the evaluated computation graph as\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "In PyTorch, we can write this as"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-HpojJ_MThQ"
      },
      "source": [
        "![tree-img](https://colah.github.io/posts/2015-08-Backprop/img/tree-eval.png)\n",
        "\n",
        "[source](https://colah.github.io/posts/2015-08-Backprop/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7NGX7CVMThR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095901dc-972a-4e50-e09c-5bb9e1ff55aa"
      },
      "source": [
        "# 记住graph又有何用？require grad里的grad是gradients的意思 \n",
        "a = torch.tensor(2.0, requires_grad=True) # we set requires_grad=True to let PyTorch know to keep the graph\n",
        "b = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "c = a + b\n",
        "d = b + 1\n",
        "e = c * d\n",
        "\n",
        "print('c', c)\n",
        "print('d', d)\n",
        "print('e', e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2., requires_grad=True)\n",
            "tensor(2., requires_grad=True)\n",
            "c tensor(3., grad_fn=<AddBackward0>)\n",
            "d tensor(2., grad_fn=<AddBackward0>)\n",
            "e tensor(6., grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orGtJTjkMThU"
      },
      "source": [
        "We can see that PyTorch kept track of the computation graph for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPZfJ1hy4uxj"
      },
      "source": [
        "## CUDA SEMANTICS\n",
        "It's easy cupy tensor from cpu to gpu or from gpu to cpu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYqe5vVv43tG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "202de93f-1f82-4ee4-9c53-0ec985655323"
      },
      "source": [
        "cpu = torch.device(\"cpu\")\n",
        "gpu = torch.device(\"cuda\")\n",
        "\n",
        "x = torch.rand(10)\n",
        "print(x)\n",
        "x = x.to(gpu)\n",
        "print(x)\n",
        "x = x.to(cpu)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.3959, 0.6177, 0.7256, 0.0971, 0.9186, 0.8277, 0.4409, 0.9344, 0.8967,\n",
            "        0.1897])\n",
            "tensor([0.3959, 0.6177, 0.7256, 0.0971, 0.9186, 0.8277, 0.4409, 0.9344, 0.8967,\n",
            "        0.1897], device='cuda:0')\n",
            "tensor([0.3959, 0.6177, 0.7256, 0.0971, 0.9186, 0.8277, 0.4409, 0.9344, 0.8967,\n",
            "        0.1897])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7Wy2mEOMThU"
      },
      "source": [
        "## PyTorch as an auto grad framework\n",
        "\n",
        "Now that we have seen that PyTorch keeps the graph around for us, let's use it to compute some gradients for us.\n",
        "\n",
        "Consider the function $f(x) = (x-2)^2$.\n",
        "\n",
        "Q: Compute $\\frac{d}{dx} f(x)$ and then compute $f'(1)$.\n",
        "\n",
        "We make a `backward()` call on the leaf variable (`y`) in the computation, computing all the gradients of `y` at once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvN0jSOKMThV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0671305b-2808-4f8b-f2c1-565b8efdcbf2"
      },
      "source": [
        "# 什么意思？你这个demo里的y很鸡肋啊。除了展示可以有backword的操作，但这个操作到底有什么效果？backward就是算gradients，假设是x是最开始的variable, y是x经过一系列变形后的结果，y.backward()算出的是dy/dx当x等于现在值时的值。\n",
        "# 如果有y1，y2，y3等等好几个x的函数，然后再进行好几次backward：y1.backward()，y2.backward(),y3.backward(),其实最后算出的就是|dy1/dx,dy2/dx,dy3/dx|当x等于目前给他赋予的数字时的值\n",
        "def f(x):\n",
        "    return (x-2)**3\n",
        "\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "y = (x-2)**3\n",
        "yy = (x-2)**4\n",
        "yyy = (x-2)**5\n",
        "\n",
        "print('PyTorch\\'s f\\'(x):', x.grad)\n",
        "yyy.backward()\n",
        "yy.backward()\n",
        "y.backward()\n",
        "\n",
        "print('PyTorch\\'s f\\'(x):', x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch's f'(x): None\n",
            "PyTorch's f'(x): tensor([4.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvJR6H7KMThX"
      },
      "source": [
        "It can also find gradients of functions.\n",
        "\n",
        "Let $w = [w_1, w_2]^T$\n",
        "\n",
        "Consider $g(w) = 2w_1w_2 + w_2\\cos(w_1)$\n",
        "\n",
        "Q: Compute $\\nabla_w g(w)$ and verify $\\nabla_w g([\\pi,1]) = [2, \\pi - 1]^T$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WCp53C1MThY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2d448c41-cb28-46c1-e072-5095a1dd82aa"
      },
      "source": [
        "def g(w):\n",
        "    return 2*w[0]*w[1] + w[1]*torch.cos(w[0])\n",
        "\n",
        "def grad_g(w):\n",
        "    return torch.tensor([2*w[1] - w[1]*torch.sin(w[0]), 2*w[0] + torch.cos(w[0])])\n",
        "\n",
        "w = torch.tensor([np.pi, 1], requires_grad=True)\n",
        "\n",
        "z = g(w)\n",
        "z.backward()\n",
        "\n",
        "print('Analytical grad g(w)', grad_g(w))\n",
        "print('PyTorch\\'s grad g(w)', w.grad)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analytical grad g(w) tensor([2.0000, 5.2832])\n",
            "PyTorch's grad g(w) tensor([2.0000, 5.2832])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqyBFdobMTha"
      },
      "source": [
        "## Using the gradients\n",
        "Now that we have gradients, we can use our favorite optimization algorithm: gradient descent!\n",
        "\n",
        "Let $f$ the same function we defined above.\n",
        "\n",
        "Q: What is the value of $x$ that minimizes $f$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4-8fhqAMThb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "98f1d676-b28f-4269-8cd0-17a405d9a8f8"
      },
      "source": [
        "x = torch.tensor([5.0], requires_grad=True)\n",
        "step_size = 0.25\n",
        "\n",
        "print('iter,\\tx,\\tf(x),\\tf\\'(x),\\tf\\'(x) pytorch')\n",
        "for i in range(15):\n",
        "    y = f(x)\n",
        "    y.backward() # compute the gradient\n",
        "    \n",
        "    print('{},\\t{:.3f},\\t{:.3f},\\t{:.3f},\\t{:.3f}'.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))\n",
        "    \n",
        "    x.data = x.data - step_size * x.grad # perform a GD update step\n",
        "    \n",
        "    # We need to zero the grad variable since the backward()\n",
        "    # call accumulates the gradients in .grad instead of overwriting.\n",
        "    # The detach_() is for efficiency. You do not need to worry too much about it.\n",
        "    x.grad.detach_()\n",
        "    x.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter,\tx,\tf(x),\tf'(x),\tf'(x) pytorch\n",
            "0,\t5.000,\t9.000,\t6.000,\t6.000\n",
            "1,\t3.500,\t2.250,\t3.000,\t3.000\n",
            "2,\t2.750,\t0.562,\t1.500,\t1.500\n",
            "3,\t2.375,\t0.141,\t0.750,\t0.750\n",
            "4,\t2.188,\t0.035,\t0.375,\t0.375\n",
            "5,\t2.094,\t0.009,\t0.188,\t0.188\n",
            "6,\t2.047,\t0.002,\t0.094,\t0.094\n",
            "7,\t2.023,\t0.001,\t0.047,\t0.047\n",
            "8,\t2.012,\t0.000,\t0.023,\t0.023\n",
            "9,\t2.006,\t0.000,\t0.012,\t0.012\n",
            "10,\t2.003,\t0.000,\t0.006,\t0.006\n",
            "11,\t2.001,\t0.000,\t0.003,\t0.003\n",
            "12,\t2.001,\t0.000,\t0.001,\t0.001\n",
            "13,\t2.000,\t0.000,\t0.001,\t0.001\n",
            "14,\t2.000,\t0.000,\t0.000,\t0.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TPWiwARMThd"
      },
      "source": [
        "# Linear Regression\n",
        "\n",
        "Now, instead of minimizing a made-up function, lets minimize a loss function on some made-up data.\n",
        "\n",
        "We will implement Gradient Descent in order to solve the task of linear regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3el-4esEMThe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "40f1a3ea-24e9-4fd1-b93d-12882e6052c7"
      },
      "source": [
        "# make a simple linear dataset with some noise\n",
        "# @是什么？dot product？\n",
        "d = 2\n",
        "n = 50\n",
        "X = torch.randn(n,d)\n",
        "true_w = torch.tensor([[-1.0], [2.0]])\n",
        "y = X @ true_w + torch.randn(n,1) * 0.1\n",
        "print('X shape', X.shape)\n",
        "print('y shape', y.shape)\n",
        "print('w shape', true_w.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape torch.Size([50, 2])\n",
            "y shape torch.Size([50, 1])\n",
            "w shape torch.Size([2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlzoXa6UMThg"
      },
      "source": [
        "### Note: dimensions\n",
        "PyTorch does a lot of operations on batches of data. The convention is to have your data be of size $(N, d)$ where $N$ is the size of the batch of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l08nQdE9MThp"
      },
      "source": [
        "### Sanity check\n",
        "To verify PyTorch is computing the gradients correctly, let's recall the gradient for the RSS objective:\n",
        "\n",
        "$$\\nabla_w \\mathcal{L}_{RSS}(w; X) = \\nabla_w\\frac{1}{n} ||y - Xw||_2^2 = -\\frac{2}{n}X^T(y-Xw)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5HfA5YcMThp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cad02754-6dde-4c34-c4dc-b273bcf2e614"
      },
      "source": [
        "# define a linear model with no bias\n",
        "def model(X, w):\n",
        "    return X @ w\n",
        "\n",
        "# the residual sum of squares loss function\n",
        "def rss(y, y_hat):\n",
        "    return torch.norm(y - y_hat)**2 / n\n",
        "\n",
        "# analytical expression for the gradient\n",
        "def grad_rss(X, y, w):\n",
        "    return -2*X.t() @ (y - X @ w) / n\n",
        "\n",
        "w = torch.tensor([[1.], [0]], requires_grad=True)\n",
        "y_hat = model(X, w)\n",
        "\n",
        "loss = rss(y, y_hat)\n",
        "loss.backward()\n",
        "\n",
        "print('Analytical gradient', grad_rss(X, y, w).detach().view(2).numpy())\n",
        "print('PyTorch\\'s gradient', w.grad.view(2).numpy())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analytical gradient [ 4.342543  -3.5023162]\n",
            "PyTorch's gradient [ 4.342543 -3.502316]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmg4eFQAMThr"
      },
      "source": [
        "Now that we've seen PyTorch is doing the right think, let's use the gradients!\n",
        "\n",
        "## Linear regression using GD with automatically computed derivatives\n",
        "\n",
        "We will now use the gradients to run the gradient descent algorithm.\n",
        "\n",
        "Note: This example is an illustration to connect ideas we have seen before to PyTorch's way of doing things. We will see how to do this in the \"PyTorchic\" way in the next example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gea4LETnMThs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "e9701f26-7f06-4bc2-80c2-1cb2b68ea666"
      },
      "source": [
        "step_size = 0.1\n",
        "\n",
        "print('iter,\\tloss,\\tw')\n",
        "for i in range(20):\n",
        "    y_hat = model(X, w)\n",
        "    loss = rss(y, y_hat)\n",
        "    \n",
        "    loss.backward() # compute the gradient of the loss\n",
        "    \n",
        "    w.data = w.data - step_size * w.grad # do a gradient descent step\n",
        "    \n",
        "    print('{},\\t{:.2f},\\t{}'.format(i, loss.item(), w.view(2).detach().numpy()))\n",
        "    \n",
        "    # We need to zero the grad variable since the backward()\n",
        "    # call accumulates the gradients in .grad instead of overwriting.\n",
        "    # The detach_() is for efficiency. You do not need to worry too much about it.\n",
        "    w.grad.detach()\n",
        "    w.grad.zero_()\n",
        "\n",
        "print('\\ntrue w\\t\\t', true_w.view(2).numpy())\n",
        "print('estimated w\\t', w.view(2).detach().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter,\tloss,\tw\n",
            "0,\t7.82,\t[0.13149136 0.70046324]\n",
            "1,\t2.84,\t[-0.11822014  0.9229876 ]\n",
            "2,\t1.84,\t[-0.31444427  1.1054724 ]\n",
            "3,\t1.19,\t[-0.4684834  1.2552956]\n",
            "4,\t0.77,\t[-0.58927345  1.3784461 ]\n",
            "5,\t0.50,\t[-0.68387645  1.4797904 ]\n",
            "6,\t0.33,\t[-0.75787055  1.563287  ]\n",
            "7,\t0.22,\t[-0.8156596  1.632159 ]\n",
            "8,\t0.15,\t[-0.86071837  1.6890337 ]\n",
            "9,\t0.10,\t[-0.89578694  1.736055  ]\n",
            "10,\t0.07,\t[-0.9230244  1.7749742]\n",
            "11,\t0.05,\t[-0.94413096  1.8072236 ]\n",
            "12,\t0.03,\t[-0.9604442  1.8339758]\n",
            "13,\t0.02,\t[-0.9730157  1.8561921]\n",
            "14,\t0.02,\t[-0.9826713  1.8746614]\n",
            "15,\t0.01,\t[-0.99005884  1.8900318 ]\n",
            "16,\t0.01,\t[-0.99568594  1.9028363 ]\n",
            "17,\t0.01,\t[-0.99994993  1.913514  ]\n",
            "18,\t0.01,\t[-1.0031612  1.9224268]\n",
            "19,\t0.01,\t[-1.0055621  1.9298735]\n",
            "\n",
            "true w\t\t [-1.  2.]\n",
            "estimated w\t [-1.0055621  1.9298735]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AexdjJtcMThu"
      },
      "source": [
        "## torch.nn.Module\n",
        "\n",
        "`Module` is PyTorch's way of performing operations on tensors. Modules are implemented as subclasses of the `torch.nn.Module` class. All modules are callable and can be composed together to create complex functions.\n",
        "\n",
        "[`torch.nn` docs](https://pytorch.org/docs/stable/nn.html)\n",
        "\n",
        "Note: most of the functionality implemented for modules can be accessed in a functional form via `torch.nn.functional`, but these require you to create and manage the weight tensors yourself.\n",
        "\n",
        "[`torch.nn.functional` docs](https://pytorch.org/docs/stable/nn.html#torch-nn-functional)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuigjBAiMThv"
      },
      "source": [
        "### Linear Module\n",
        "The bread and butter of modules is the Linear module which does a linear transformation with a bias. It takes the input and output dimensions as parameters, and creates the weights in the object.\n",
        "\n",
        "Unlike how we initialized our $w$ manually, the Linear module automatically initializes the weights randomly. For minimizing non convex loss functions (e.g. training neural networks), initialization is important and can affect results. If training isn't working as well as expected, one thing to try is manually initializing the weights to something different from the default. PyTorch implements some common initializations in `torch.nn.init`.\n",
        "\n",
        "[`torch.nn.init` docs](https://pytorch.org/docs/stable/nn.html#torch-nn-init)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi4lhPVCMThv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f087db-8139-45c7-dc61-20ab3d034885"
      },
      "source": [
        "# pytorch linear_module 把2x3 变成 2x5, 那weights岂不是需要乘以3x5,怎么是5X3? \n",
        "# 网上搜了一下，就应该是3x5,weights其实是transpose后的结果\n",
        "d_in = 3\n",
        "d_out = 5\n",
        "linear_module = nn.Linear(d_in, d_out)\n",
        "\n",
        "example_tensor = torch.tensor([[1.,2,3], [4,5,6]])\n",
        "# applys a linear transformation to the data\n",
        "transformed = linear_module(example_tensor)\n",
        "print('example_tensor', example_tensor)\n",
        "print('transormed', transformed)\n",
        "print()\n",
        "print('We can see that the weights exist in the background\\n')\n",
        "print('W:')\n",
        "params = linear_module.parameters()\n",
        "for param in params:\n",
        "    print(param)\n",
        "print('b:', linear_module.bias)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example_tensor tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "transormed tensor([[-0.7032, -0.8563, -1.0732,  0.7261,  2.2426],\n",
            "        [-1.9358, -1.0281, -2.6585,  0.8080,  4.9279]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "\n",
            "We can see that the weights exist in the background\n",
            "\n",
            "W:\n",
            "Parameter containing:\n",
            "tensor([[ 0.2880, -0.5320, -0.1669],\n",
            "        [ 0.5167, -0.0162, -0.5578],\n",
            "        [-0.0987, -0.5202,  0.0905],\n",
            "        [-0.0967, -0.4257,  0.5497],\n",
            "        [ 0.3375,  0.1064,  0.4512]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.5734,  0.3327, -0.2054,  0.0251,  0.3387], requires_grad=True)\n",
            "b: Parameter containing:\n",
            "tensor([ 0.5734,  0.3327, -0.2054,  0.0251,  0.3387], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLNmKz9BMThx"
      },
      "source": [
        "### Activation functions\n",
        "PyTorch implements a number of activation functions including but not limited to `ReLU`, `Tanh`, and `Sigmoid`. Since they are modules, they need to be instantiated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toOsF9qXMThy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d8345bf-b42b-48d6-a5bd-8adab011b0b2"
      },
      "source": [
        "activation_fn = nn.ReLU() # we instantiate an instance of the ReLU module\n",
        "example_tensor = torch.tensor([-1.0, 1.0, 0.0])\n",
        "activated = activation_fn(example_tensor)\n",
        "print('example_tensor', example_tensor)\n",
        "print('activated', activated)\n",
        "print('weight:')\n",
        "for i in activation_fn.parameters():\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example_tensor tensor([-1.,  1.,  0.])\n",
            "activated tensor([0., 1., 0.])\n",
            "weight:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPdu_KS_MTh0"
      },
      "source": [
        "### Sequential\n",
        "\n",
        "Many times, we want to compose Modules together. `torch.nn.Sequential` provides a good interface for composing simple modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn-jaKd3MTh1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1dd86d16-7560-4882-c7bf-c8b3f1e63fbd"
      },
      "source": [
        "d_in = 3\n",
        "d_hidden = 4\n",
        "d_out = 1\n",
        "model = torch.nn.Sequential(\n",
        "                            nn.Linear(d_in, d_hidden),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(d_hidden, d_out),\n",
        "                            nn.Sigmoid()\n",
        "                           )\n",
        "\n",
        "example_tensor = torch.tensor([[1.,2,3],[4,5,6]])\n",
        "transformed = model(example_tensor)\n",
        "print('transformed', transformed.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transformed torch.Size([2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5GkJ1UTMTh2"
      },
      "source": [
        "Note: we can access *all* of the parameters (of any `nn.Module`) with the `parameters()` method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTTsMkxoMTh3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "6b52d368-18a1-4486-8735-00d82482dc2b"
      },
      "source": [
        "# 怎么tanh和sigmoid还有weights？\n",
        "# 用来这四组不都是weights，不是给每一个部分都有weights，只有其中的两个linear transformation有。显示的四组tensors分别是两个linear transformation的weights和bias，两组weights，两组bias\n",
        "params = model.parameters()\n",
        "\n",
        "for param in params:\n",
        "    print(param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.5607,  0.4221, -0.0254],\n",
            "        [-0.3630,  0.4541,  0.0275],\n",
            "        [-0.0703, -0.1463,  0.3065],\n",
            "        [ 0.0065, -0.2664,  0.0267]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.3196,  0.2911,  0.1999, -0.3758], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0289,  0.1544,  0.3992, -0.3301]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1438], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y5HJVuuz6ey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddb4d5a-d2c3-4037-a057-f41eb95cca0b"
      },
      "source": [
        "d_in = 3\n",
        "d_hidden = 4\n",
        "d_out = 1\n",
        "l1 = nn.Linear(d_in, d_hidden)\n",
        "model = torch.nn.Sequential(\n",
        "                            l1,\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(d_hidden, d_out),\n",
        "                            nn.Sigmoid()\n",
        "                           )\n",
        "transformed = model(example_tensor)\n",
        "params = model.parameters()\n",
        "\n",
        "for param in params:\n",
        "    print(param)\n",
        "\n",
        "\n",
        "print(\"l1 bias\",l1.bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.1181,  0.5671,  0.3901],\n",
            "        [-0.0432,  0.2027,  0.3270],\n",
            "        [ 0.2183,  0.2269, -0.5094],\n",
            "        [-0.4306,  0.2483, -0.0776]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.5372,  0.0966, -0.1610,  0.2270], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0054,  0.1198, -0.1696, -0.2241]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0306], requires_grad=True)\n",
            "l1 bias Parameter containing:\n",
            "tensor([-0.5372,  0.0966, -0.1610,  0.2270], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jifMOIcNMTh5"
      },
      "source": [
        "### Loss functions\n",
        "PyTorch implements many common loss functions including `MSELoss` and `CrossEntropyLoss`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8NXNEhlMTh6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "24e9a149-87e7-48a4-f96f-1ee6a14f0f86"
      },
      "source": [
        "mse_loss_fn = nn.MSELoss()\n",
        "\n",
        "input = torch.tensor([[0., 0, 0]])\n",
        "target = torch.tensor([[1., 0, -1]])\n",
        "\n",
        "loss = mse_loss_fn(input, target)\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.6667)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh0YZh1QMTh7"
      },
      "source": [
        "## torch.optim\n",
        "PyTorch implements a number of gradient-based optimization methods in `torch.optim`, including Gradient Descent. At the minimum, it takes in the model parameters and a learning rate.\n",
        "\n",
        "Optimizers do not compute the gradients for you, so you must call `backward()` yourself. You also must call the `optim.zero_grad()` function before calling `backward()` since by default PyTorch does and inplace add to the `.grad` member variable rather than overwriting it.\n",
        "\n",
        "This does both the `detach_()` and `zero_()` calls on all tensor's `grad` variables.\n",
        "\n",
        "[`torch.optim` docs](https://pytorch.org/docs/stable/optim.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CldNJzMHMTh8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c62667a6-2b2a-4191-85d2-84e3ba51f1be"
      },
      "source": [
        "# create a simple model\n",
        "model = nn.Linear(1, 1)\n",
        "\n",
        "# create a simple dataset\n",
        "X_simple = torch.tensor([[1.]])\n",
        "y_simple = torch.tensor([[2.]])\n",
        "\n",
        "# create our optimizer\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
        "mse_loss_fn = nn.MSELoss()\n",
        "\n",
        "y_hat = model(X_simple)\n",
        "print('model params before:', model.weight)\n",
        "loss = mse_loss_fn(y_hat, y_simple)\n",
        "optim.zero_grad()\n",
        "loss.backward()\n",
        "optim.step()\n",
        "print('model params after:', model.weight)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model params before: Parameter containing:\n",
            "tensor([[-0.9604]], requires_grad=True)\n",
            "model params after: Parameter containing:\n",
            "tensor([[-0.9060]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxv9VHTOMTh-"
      },
      "source": [
        "As we can see, the parameter was updated in the correct direction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjiD9FATMTh_"
      },
      "source": [
        "## Linear regression using GD with automatically computed derivatives and PyTorch's Modules\n",
        "\n",
        "Now let's combine what we've learned to solve linear regression in a \"PyTorchic\" way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGz8gPweMTh_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "e3401d7e-9f8a-4e42-96e5-dfb00c32ccf6"
      },
      "source": [
        "step_size = 0.1\n",
        "\n",
        "linear_module = nn.Linear(d, 1, bias=False)\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)\n",
        "\n",
        "print('iter,\\tloss,\\tw')\n",
        "\n",
        "for i in range(20):\n",
        "    y_hat = linear_module(X)\n",
        "    loss = loss_func(y_hat, y)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    \n",
        "    print('{},\\t{:.2f},\\t{}'.format(i, loss.item(), linear_module.weight.view(2).detach().numpy()))\n",
        "\n",
        "print('\\ntrue w\\t\\t', true_w.view(2).numpy())\n",
        "print('estimated w\\t', linear_module.weight.view(2).detach().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter,\tloss,\tw\n",
            "0,\t6.14,\t[-0.4951109  -0.20055914]\n",
            "1,\t4.19,\t[-0.64017504  0.1509075 ]\n",
            "2,\t2.87,\t[-0.7496651  0.4441856]\n",
            "3,\t1.98,\t[-0.8317375  0.689143 ]\n",
            "4,\t1.37,\t[-0.8927491   0.89393103]\n",
            "5,\t0.95,\t[-0.93764454  1.0652909 ]\n",
            "6,\t0.67,\t[-0.9702622  1.208804 ]\n",
            "7,\t0.47,\t[-0.99357456  1.3290964 ]\n",
            "8,\t0.33,\t[-1.0098771  1.4300069]\n",
            "9,\t0.23,\t[-1.0209374  1.5147243]\n",
            "10,\t0.17,\t[-1.028112   1.5859002]\n",
            "11,\t0.12,\t[-1.0324373  1.6457422]\n",
            "12,\t0.09,\t[-1.0347017  1.6960896]\n",
            "13,\t0.06,\t[-1.035502   1.7384766]\n",
            "14,\t0.05,\t[-1.0352864  1.7741843]\n",
            "15,\t0.04,\t[-1.0343897  1.8042834]\n",
            "16,\t0.03,\t[-1.033059  1.829669]\n",
            "17,\t0.02,\t[-1.031475   1.8510911]\n",
            "18,\t0.02,\t[-1.0297676  1.8691778]\n",
            "19,\t0.01,\t[-1.0280287  1.8844559]\n",
            "\n",
            "true w\t\t [-1.  2.]\n",
            "estimated w\t [-1.0280287  1.8844559]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j9hUvjQMTiD"
      },
      "source": [
        "## Linear regression using SGD \n",
        "In the previous examples, we computed the average gradient over the entire dataset (Gradient Descent). We can implement Stochastic Gradient Descent with a simple modification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3EUcFMbMTiE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "34a3f055-cefa-4bc2-b6fb-13b3117b620c"
      },
      "source": [
        "# gradient descent 和 stochastic gradient descent 就是算gradient时loss一个是用所有x算的，另一个只用了一个x\n",
        "step_size = 0.01\n",
        "\n",
        "linear_module = nn.Linear(d, 1)\n",
        "loss_func = nn.MSELoss()\n",
        "optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)\n",
        "print('iter,\\tloss,\\tw')\n",
        "for i in range(200):\n",
        "    rand_idx = np.random.choice(n) # take a random point from the dataset\n",
        "    x = X[rand_idx] \n",
        "    y_hat = linear_module(x)\n",
        "    loss = loss_func(y_hat, y[rand_idx]) # only compute the loss on the single point\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    \n",
        "    if i % 20 == 0:\n",
        "        print('{},\\t{:.2f},\\t{}'.format(i, loss.item(), linear_module.weight.view(2).detach().numpy()))\n",
        "\n",
        "print('\\ntrue w\\t\\t', true_w.view(2).numpy())\n",
        "print('estimated w\\t', linear_module.weight.view(2).detach().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter,\tloss,\tw\n",
            "0,\t5.33,\t[-0.52818084  0.2690754 ]\n",
            "20,\t1.33,\t[-0.5849738   0.54701847]\n",
            "40,\t0.21,\t[-0.68336743  0.93094164]\n",
            "60,\t0.41,\t[-0.76554966  1.3865377 ]\n",
            "80,\t0.22,\t[-0.8548197  1.528812 ]\n",
            "100,\t0.45,\t[-0.9011376  1.679943 ]\n",
            "120,\t0.04,\t[-0.9418524  1.7858417]\n",
            "140,\t0.00,\t[-0.97288156  1.857902  ]\n",
            "160,\t0.00,\t[-0.98335326  1.893024  ]\n",
            "180,\t0.01,\t[-0.9927237  1.904962 ]\n",
            "\n",
            "true w\t\t [-1.  2.]\n",
            "estimated w\t [-0.99158174  1.9331173 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Re8u8STMTiI"
      },
      "source": [
        "# Neural Network Basics in PyTorch\n",
        "We will try and fit a simple neural network to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "401On5ckMTiJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9929b87-bee5-40aa-a3ee-60632fabf110"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "d = 1\n",
        "n = 200\n",
        "X = torch.rand(n,d)\n",
        "y = 4 * torch.sin(np.pi * X) * torch.cos(6*np.pi*X**2)\n",
        "print(X)\n",
        "print(y)\n",
        "plt.scatter(X.numpy(), y.numpy())\n",
        "plt.title('plot of $f(x)$')\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$y$')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2010],\n",
            "        [0.5419],\n",
            "        [0.0024],\n",
            "        [0.3200],\n",
            "        [0.1604],\n",
            "        [0.6338],\n",
            "        [0.5056],\n",
            "        [0.5847],\n",
            "        [0.8565],\n",
            "        [0.0176],\n",
            "        [0.4135],\n",
            "        [0.1011],\n",
            "        [0.5402],\n",
            "        [0.3133],\n",
            "        [0.7141],\n",
            "        [0.1675],\n",
            "        [0.5838],\n",
            "        [0.6331],\n",
            "        [0.2302],\n",
            "        [0.0330],\n",
            "        [0.2040],\n",
            "        [0.0416],\n",
            "        [0.2260],\n",
            "        [0.2483],\n",
            "        [0.9169],\n",
            "        [0.8225],\n",
            "        [0.4267],\n",
            "        [0.8418],\n",
            "        [0.4762],\n",
            "        [0.6946],\n",
            "        [0.1608],\n",
            "        [0.9399],\n",
            "        [0.1452],\n",
            "        [0.3706],\n",
            "        [0.3608],\n",
            "        [0.0253],\n",
            "        [0.1087],\n",
            "        [0.6947],\n",
            "        [0.9682],\n",
            "        [0.7935],\n",
            "        [0.6013],\n",
            "        [0.2559],\n",
            "        [0.9169],\n",
            "        [0.3298],\n",
            "        [0.3513],\n",
            "        [0.3303],\n",
            "        [0.2823],\n",
            "        [0.1627],\n",
            "        [0.2472],\n",
            "        [0.4383],\n",
            "        [0.9006],\n",
            "        [0.6461],\n",
            "        [0.5617],\n",
            "        [0.1450],\n",
            "        [0.4811],\n",
            "        [0.2799],\n",
            "        [0.6104],\n",
            "        [0.4229],\n",
            "        [0.0238],\n",
            "        [0.3950],\n",
            "        [0.3410],\n",
            "        [0.9077],\n",
            "        [0.1675],\n",
            "        [0.0588],\n",
            "        [0.6713],\n",
            "        [0.8758],\n",
            "        [0.3267],\n",
            "        [0.1689],\n",
            "        [0.1733],\n",
            "        [0.2573],\n",
            "        [0.6466],\n",
            "        [0.8861],\n",
            "        [0.8167],\n",
            "        [0.1816],\n",
            "        [0.1098],\n",
            "        [0.9730],\n",
            "        [0.6588],\n",
            "        [0.4947],\n",
            "        [0.1547],\n",
            "        [0.3688],\n",
            "        [0.9194],\n",
            "        [0.4075],\n",
            "        [0.1428],\n",
            "        [0.6983],\n",
            "        [0.1379],\n",
            "        [0.7733],\n",
            "        [0.0778],\n",
            "        [0.4971],\n",
            "        [0.6632],\n",
            "        [0.1232],\n",
            "        [0.5469],\n",
            "        [0.8115],\n",
            "        [0.4634],\n",
            "        [0.1223],\n",
            "        [0.1832],\n",
            "        [0.3011],\n",
            "        [0.3696],\n",
            "        [0.0679],\n",
            "        [0.1072],\n",
            "        [0.9881],\n",
            "        [0.9051],\n",
            "        [0.2801],\n",
            "        [0.4557],\n",
            "        [0.9989],\n",
            "        [0.1402],\n",
            "        [0.8137],\n",
            "        [0.0967],\n",
            "        [0.4736],\n",
            "        [0.5631],\n",
            "        [0.4148],\n",
            "        [0.8420],\n",
            "        [0.2187],\n",
            "        [0.5048],\n",
            "        [0.1609],\n",
            "        [0.6916],\n",
            "        [0.4334],\n",
            "        [0.4283],\n",
            "        [0.4502],\n",
            "        [0.6819],\n",
            "        [0.1385],\n",
            "        [0.8980],\n",
            "        [0.7635],\n",
            "        [0.2332],\n",
            "        [0.3833],\n",
            "        [0.4890],\n",
            "        [0.0019],\n",
            "        [0.5284],\n",
            "        [0.8105],\n",
            "        [0.9890],\n",
            "        [0.5760],\n",
            "        [0.5665],\n",
            "        [0.5503],\n",
            "        [0.2564],\n",
            "        [0.0196],\n",
            "        [0.2203],\n",
            "        [0.1602],\n",
            "        [0.1025],\n",
            "        [0.6358],\n",
            "        [0.3922],\n",
            "        [0.8682],\n",
            "        [0.2584],\n",
            "        [0.7494],\n",
            "        [0.0393],\n",
            "        [0.3555],\n",
            "        [0.9475],\n",
            "        [0.4860],\n",
            "        [0.0169],\n",
            "        [0.4146],\n",
            "        [0.0495],\n",
            "        [0.5783],\n",
            "        [0.4163],\n",
            "        [0.1314],\n",
            "        [0.9760],\n",
            "        [0.7923],\n",
            "        [0.5921],\n",
            "        [0.8908],\n",
            "        [0.9965],\n",
            "        [0.7881],\n",
            "        [0.3221],\n",
            "        [0.5217],\n",
            "        [0.7933],\n",
            "        [0.8591],\n",
            "        [0.7843],\n",
            "        [0.5572],\n",
            "        [0.2246],\n",
            "        [0.9024],\n",
            "        [0.8722],\n",
            "        [0.9181],\n",
            "        [0.3633],\n",
            "        [0.5701],\n",
            "        [0.2749],\n",
            "        [0.9531],\n",
            "        [0.0779],\n",
            "        [0.6554],\n",
            "        [0.5617],\n",
            "        [0.3336],\n",
            "        [0.8160],\n",
            "        [0.3978],\n",
            "        [0.9911],\n",
            "        [0.8379],\n",
            "        [0.4626],\n",
            "        [0.6756],\n",
            "        [0.7832],\n",
            "        [0.6891],\n",
            "        [0.6965],\n",
            "        [0.0589],\n",
            "        [0.1271],\n",
            "        [0.7150],\n",
            "        [0.4328],\n",
            "        [0.0348],\n",
            "        [0.5836],\n",
            "        [0.3606],\n",
            "        [0.6966],\n",
            "        [0.4946],\n",
            "        [0.6198],\n",
            "        [0.3304],\n",
            "        [0.2759],\n",
            "        [0.5306],\n",
            "        [0.9744],\n",
            "        [0.0034]])\n",
            "tensor([[ 1.7092],\n",
            "        [ 2.9049],\n",
            "        [ 0.0305],\n",
            "        [-1.1887],\n",
            "        [ 1.7087],\n",
            "        [ 1.0188],\n",
            "        [ 0.4269],\n",
            "        [ 3.8087],\n",
            "        [ 0.5311],\n",
            "        [ 0.2210],\n",
            "        [-3.8405],\n",
            "        [ 1.2260],\n",
            "        [ 2.8162],\n",
            "        [-0.9184],\n",
            "        [-3.0736],\n",
            "        [ 1.7347],\n",
            "        [ 3.8235],\n",
            "        [ 1.0790],\n",
            "        [ 1.4328],\n",
            "        [ 0.4135],\n",
            "        [ 1.6929],\n",
            "        [ 0.5212],\n",
            "        [ 1.4895],\n",
            "        [ 1.1180],\n",
            "        [-1.0221],\n",
            "        [ 2.0795],\n",
            "        [-3.7318],\n",
            "        [ 1.3388],\n",
            "        [-1.6909],\n",
            "        [-3.0989],\n",
            "        [ 1.7105],\n",
            "        [-0.4392],\n",
            "        [ 1.6247],\n",
            "        [-3.1265],\n",
            "        [-2.7995],\n",
            "        [ 0.3179],\n",
            "        [ 1.3060],\n",
            "        [-3.0997],\n",
            "        [ 0.1524],\n",
            "        [ 1.8517],\n",
            "        [ 3.2751],\n",
            "        [ 0.9505],\n",
            "        [-1.0220],\n",
            "        [-1.5893],\n",
            "        [-2.4492],\n",
            "        [-1.6078],\n",
            "        [ 0.2112],\n",
            "        [ 1.7181],\n",
            "        [ 1.1412],\n",
            "        [-3.4835],\n",
            "        [-1.1221],\n",
            "        [-0.0527],\n",
            "        [ 3.7064],\n",
            "        [ 1.6235],\n",
            "        [-1.3689],\n",
            "        [ 0.2880],\n",
            "        [ 2.7767],\n",
            "        [-3.7814],\n",
            "        [ 0.2989],\n",
            "        [-3.7087],\n",
            "        [-2.0412],\n",
            "        [-1.1257],\n",
            "        [ 1.7347],\n",
            "        [ 0.7332],\n",
            "        [-2.0516],\n",
            "        [-0.4809],\n",
            "        [-1.4605],\n",
            "        [ 1.7387],\n",
            "        [ 1.7485],\n",
            "        [ 0.9189],\n",
            "        [-0.0996],\n",
            "        [-0.8628],\n",
            "        [ 2.1780],\n",
            "        [ 1.7561],\n",
            "        [ 1.3173],\n",
            "        [ 0.1821],\n",
            "        [-1.1295],\n",
            "        [-0.3960],\n",
            "        [ 1.6816],\n",
            "        [-3.0685],\n",
            "        [-0.9770],\n",
            "        [-3.8322],\n",
            "        [ 1.6081],\n",
            "        [-3.1602],\n",
            "        [ 1.5724],\n",
            "        [ 0.7144],\n",
            "        [ 0.9622],\n",
            "        [-0.2167],\n",
            "        [-1.4748],\n",
            "        [ 1.4485],\n",
            "        [ 3.1610],\n",
            "        [ 2.2065],\n",
            "        [-2.4485],\n",
            "        [ 1.4402],\n",
            "        [ 1.7558],\n",
            "        [-0.4473],\n",
            "        [-3.0955],\n",
            "        [ 0.8437],\n",
            "        [ 1.2911],\n",
            "        [ 0.1350],\n",
            "        [-1.1336],\n",
            "        [ 0.2843],\n",
            "        [-2.8355],\n",
            "        [ 0.0143],\n",
            "        [ 1.5896],\n",
            "        [ 2.2014],\n",
            "        [ 1.1784],\n",
            "        [-1.8564],\n",
            "        [ 3.7388],\n",
            "        [-3.8377],\n",
            "        [ 1.3327],\n",
            "        [ 1.5735],\n",
            "        [ 0.3605],\n",
            "        [ 1.7109],\n",
            "        [-3.0243],\n",
            "        [-3.6058],\n",
            "        [-3.7049],\n",
            "        [-3.0752],\n",
            "        [-2.6592],\n",
            "        [ 1.5772],\n",
            "        [-1.1012],\n",
            "        [-0.0177],\n",
            "        [ 1.3882],\n",
            "        [-3.4787],\n",
            "        [-0.8154],\n",
            "        [ 0.0235],\n",
            "        [ 2.0844],\n",
            "        [ 2.2052],\n",
            "        [ 0.1269],\n",
            "        [ 3.8849],\n",
            "        [ 3.8061],\n",
            "        [ 3.3171],\n",
            "        [ 0.9396],\n",
            "        [ 0.2459],\n",
            "        [ 1.5569],\n",
            "        [ 1.7079],\n",
            "        [ 1.2412],\n",
            "        [ 0.8411],\n",
            "        [-3.6633],\n",
            "        [-0.1168],\n",
            "        [ 0.8908],\n",
            "        [-1.1252],\n",
            "        [ 0.4920],\n",
            "        [-2.6065],\n",
            "        [-0.2290],\n",
            "        [-1.0273],\n",
            "        [ 0.2123],\n",
            "        [-3.8383],\n",
            "        [ 0.6184],\n",
            "        [ 3.8786],\n",
            "        [-3.8324],\n",
            "        [ 1.5201],\n",
            "        [ 0.1888],\n",
            "        [ 1.8034],\n",
            "        [ 3.6320],\n",
            "        [-0.9833],\n",
            "        [ 0.0431],\n",
            "        [ 1.6163],\n",
            "        [-1.2737],\n",
            "        [ 1.6204],\n",
            "        [ 1.8439],\n",
            "        [ 0.3841],\n",
            "        [ 1.4140],\n",
            "        [ 3.5744],\n",
            "        [ 1.5075],\n",
            "        [-1.1306],\n",
            "        [-0.3119],\n",
            "        [-1.0006],\n",
            "        [-2.8886],\n",
            "        [ 3.8558],\n",
            "        [ 0.4420],\n",
            "        [-0.0912],\n",
            "        [ 0.9632],\n",
            "        [-0.8522],\n",
            "        [ 3.7067],\n",
            "        [-1.7431],\n",
            "        [ 2.1857],\n",
            "        [-3.7473],\n",
            "        [ 0.1052],\n",
            "        [ 1.5337],\n",
            "        [-2.4924],\n",
            "        [-2.3197],\n",
            "        [ 1.3513],\n",
            "        [-2.9489],\n",
            "        [-3.1341],\n",
            "        [ 0.7342],\n",
            "        [ 1.4834],\n",
            "        [-3.0512],\n",
            "        [-3.6186],\n",
            "        [ 0.4359],\n",
            "        [ 3.8263],\n",
            "        [-2.7915],\n",
            "        [-3.1351],\n",
            "        [-0.4070],\n",
            "        [ 2.1381],\n",
            "        [-1.6105],\n",
            "        [ 0.4121],\n",
            "        [ 2.2284],\n",
            "        [ 0.1862],\n",
            "        [ 0.0427]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQl2A_XDMTiL"
      },
      "source": [
        "Here we define a simple two hidden layer neural network with Tanh activations. There are a few hyper parameters to play with to get a feel for how they change the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C72y6DtpMTiM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "e91ae971-a87f-430e-ab35-1918173b0b4f"
      },
      "source": [
        "# feel free to play with these parameters\n",
        "\n",
        "step_size = 0.05\n",
        "n_epochs = 6000\n",
        "n_hidden_1 = 32\n",
        "n_hidden_2 = 32\n",
        "d_out = 1\n",
        "\n",
        "neural_network = nn.Sequential(\n",
        "                            nn.Linear(d, n_hidden_1), \n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(n_hidden_1, n_hidden_2),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(n_hidden_2, d_out)\n",
        "                            )\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "optim = torch.optim.SGD(neural_network.parameters(), lr=step_size)\n",
        "print('iter,\\tloss')\n",
        "for i in range(n_epochs):\n",
        "    y_hat = neural_network(X)\n",
        "    loss = loss_func(y_hat, y)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    \n",
        "    if i % (n_epochs // 10) == 0:\n",
        "        print('{},\\t{:.2f}'.format(i, loss.item()))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter,\tloss\n",
            "0,\t4.33\n",
            "600,\t4.27\n",
            "1200,\t3.87\n",
            "1800,\t1.54\n",
            "2400,\t0.71\n",
            "3000,\t0.78\n",
            "3600,\t0.24\n",
            "4200,\t0.10\n",
            "4800,\t0.08\n",
            "5400,\t0.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQBkFt9LMTiO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "c4353831-4f51-4087-fcaf-41b21cca1b73"
      },
      "source": [
        "X_grid = torch.from_numpy(np.linspace(0,1,50)).float().view(-1, d)\n",
        "y_hat = neural_network(X_grid)\n",
        "plt.scatter(X.numpy(), y.numpy())\n",
        "plt.plot(X_grid.detach().numpy(), y_hat.detach().numpy(), 'r')\n",
        "plt.title('plot of $f(x)$ and $\\hat{f}(x)$')\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$y$')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEdCAYAAAABymAfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8mPkng3MTiQ"
      },
      "source": [
        "# Things that might help on the homework\n",
        "\n",
        "## Brief Sidenote: Momentum\n",
        "\n",
        "There are other optimization algorithms besides stochastic gradient descent. One is a modification of SGD called momentum. We won't get into it here, but if you would like to read more [here](https://distill.pub/2017/momentum/) is a good place to start.\n",
        "\n",
        "We only change the step size and add the momentum keyword argument to the optimizer. Notice how it reduces the training loss in fewer iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhGP8gZDMTiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fdd3395-284c-4d20-c892-8593978764eb"
      },
      "source": [
        "# feel free to play with these parameters\n",
        "\n",
        "step_size = 0.05\n",
        "momentum = 0.9\n",
        "n_epochs = 1500\n",
        "n_hidden_1 = 32\n",
        "n_hidden_2 = 32\n",
        "d_out = 1\n",
        "\n",
        "neural_network = nn.Sequential(\n",
        "                            nn.Linear(d, n_hidden_1), \n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(n_hidden_1, n_hidden_2),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(n_hidden_2, d_out)\n",
        "                            )\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "optim = torch.optim.SGD(neural_network.parameters(), lr=step_size, momentum=momentum)\n",
        "print('iter,\\tloss')\n",
        "for i in range(n_epochs):\n",
        "    y_hat = neural_network(X)\n",
        "    loss = loss_func(y_hat, y)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    \n",
        "    if i % (n_epochs // 10) == 0:\n",
        "        print('{},\\t{:.2f}'.format(i, loss.item()))\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter,\tloss\n",
            "0,\t4.30\n",
            "150,\t1.87\n",
            "300,\t0.46\n",
            "450,\t0.15\n",
            "600,\t0.07\n",
            "750,\t0.05\n",
            "900,\t0.01\n",
            "1050,\t0.01\n",
            "1200,\t0.00\n",
            "1350,\t0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGZL4mkbMTiS",
        "outputId": "a47a0eb5-8e93-4538-8506-cda2ad4ce633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "X_grid = torch.from_numpy(np.linspace(0,1,50)).float().view(-1, d)\n",
        "y_hat = neural_network(X_grid)\n",
        "plt.scatter(X.numpy(), y.numpy())\n",
        "plt.plot(X_grid.detach().numpy(), y_hat.detach().numpy(), 'r')\n",
        "plt.title('plot of $f(x)$ and $\\hat{f}(x)$')\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$y$')\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSO1McZLMTiT"
      },
      "source": [
        "## CrossEntropyLoss\n",
        "So far, we have been considering regression tasks and have used the [MSELoss](https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss) module. For the homework, we will be performing a classification task and will use the cross entropy loss.\n",
        "\n",
        "PyTorch implements a version of the cross entropy loss in one module called [CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss). Its usage is slightly different than MSE, so we will break it down here. \n",
        "\n",
        "- input: The first parameter to CrossEntropyLoss is the output of our network. It expects a *real valued* tensor of dimensions $(N,C)$ where $N$ is the minibatch size and $C$ is the number of classes. In our case $N=3$ and $C=2$. The values along the second dimension correspond to raw unnormalized scores for each class. The CrossEntropyLoss module does the softmax calculation for us, so we do not need to apply our own softmax to the output of our neural network.\n",
        "- output: The second parameter to CrossEntropyLoss is the true label. It expects an *integer valued* tensor of dimension $(N)$. The integer at each element corresponds to the correct class. In our case, the \"correct\" class labels are class 0, class 1, and class 1.\n",
        "\n",
        "Try out the loss function on three toy predictions. The true class labels are $y=[1,1,0]$. The first two examples correspond to predictions that are \"correct\" in that they have higher raw scores for the correct class. The second example is \"more confident\" in the prediction, leading to a smaller loss. The last two examples are incorrect predictions with lower and higher confidence respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALoGYsu1MTiU",
        "outputId": "4a7f4c3d-8e06-45e4-bef1-72ddc36421c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "input = torch.tensor([[-1., 1],[-1, 1],[1, -1]]) # raw scores correspond to the correct class\n",
        "# input = torch.tensor([[-3., 3],[-3, 3],[3, -3]]) # raw scores correspond to the correct class with higher confidence\n",
        "# input = torch.tensor([[1., -1],[1, -1],[-1, 1]]) # raw scores correspond to the incorrect class\n",
        "# input = torch.tensor([[3., -3],[3, -3],[-3, 3]]) # raw scores correspond to the incorrect class with incorrectly placed confidence\n",
        "\n",
        "target = torch.tensor([1, 1, 0])\n",
        "output = loss(input, target)\n",
        "print(output)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.1269)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCwLf9C2MTiY"
      },
      "source": [
        "## Learning rate schedulers\n",
        "\n",
        "Often we do not want to use a fixed learning rate throughout all training. PyTorch offers learning rate schedulers to change the learning rate over time. Common strategies include multiplying the lr by a constant every epoch (e.g. 0.9) and halving the learning rate when the training loss flattens out.\n",
        "\n",
        "See the [learning rate scheduler docs](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) for usage and examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrapEC2XMTiY"
      },
      "source": [
        "## Convolutions\n",
        "When working with images, we often want to use convolutions to extract features using convolutions. PyTorch implments this for us in the `torch.nn.Conv2d` module. It expects the input to have a specific dimension $(N, C_{in}, H_{in}, W_{in})$ where $N$ is batch size, $C_{in}$ is the number of channels the image has, and $H_{in}, W_{in}$ are the image height and width respectively.\n",
        "\n",
        "We can modify the convolution to have different properties with the parameters:\n",
        "- kernel_size\n",
        "- stride\n",
        "- padding\n",
        "\n",
        "They can change the output dimension so be careful.\n",
        "\n",
        "See the [`torch.nn.Conv2d` docs](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gKSeJYuMTiZ"
      },
      "source": [
        "To illustrate what the `Conv2d` module is doing, let's set the conv weights manually to a Gaussian blur kernel.\n",
        "\n",
        "We can see that it applies the kernel to the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJjlv1lOMTiZ",
        "outputId": "57deb85b-ae64-46e3-8c48-cc51e6f44c2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# an entire mnist digit\n",
        "image = np.array([0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.3803922 , 0.37647063, 0.3019608 ,0.46274513, 0.2392157 , 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.3529412 , 0.5411765 , 0.9215687 ,0.9215687 , 0.9215687 , 0.9215687 , 0.9215687 , 0.9215687 ,0.9843138 , 0.9843138 , 0.9725491 , 0.9960785 , 0.9607844 ,0.9215687 , 0.74509805, 0.08235294, 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.54901963,0.9843138 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.7411765 , 0.09019608, 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.8862746 , 0.9960785 , 0.81568635,0.7803922 , 0.7803922 , 0.7803922 , 0.7803922 , 0.54509807,0.2392157 , 0.2392157 , 0.2392157 , 0.2392157 , 0.2392157 ,0.5019608 , 0.8705883 , 0.9960785 , 0.9960785 , 0.7411765 ,0.08235294, 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.14901961, 0.32156864, 0.0509804 , 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.13333334,0.8352942 , 0.9960785 , 0.9960785 , 0.45098042, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.32941177, 0.9960785 ,0.9960785 , 0.9176471 , 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0.32941177, 0.9960785 , 0.9960785 , 0.9176471 ,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.4156863 , 0.6156863 ,0.9960785 , 0.9960785 , 0.95294124, 0.20000002, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0.09803922, 0.45882356, 0.8941177 , 0.8941177 ,0.8941177 , 0.9921569 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.94117653, 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.26666668, 0.4666667 , 0.86274517,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.5568628 ,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.14509805, 0.73333335,0.9921569 , 0.9960785 , 0.9960785 , 0.9960785 , 0.8745099 ,0.8078432 , 0.8078432 , 0.29411766, 0.26666668, 0.8431373 ,0.9960785 , 0.9960785 , 0.45882356, 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.4431373 , 0.8588236 , 0.9960785 , 0.9490197 , 0.89019614,0.45098042, 0.34901962, 0.12156864, 0., 0.,0., 0., 0.7843138 , 0.9960785 , 0.9450981 ,0.16078432, 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.6627451 , 0.9960785 ,0.6901961 , 0.24313727, 0., 0., 0.,0., 0., 0., 0., 0.18823531,0.9058824 , 0.9960785 , 0.9176471 , 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0.07058824, 0.48627454, 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.32941177, 0.9960785 , 0.9960785 ,0.6509804 , 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.54509807, 0.9960785 , 0.9333334 , 0.22352943, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.8235295 , 0.9803922 , 0.9960785 ,0.65882355, 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.9490197 , 0.9960785 , 0.93725497, 0.22352943, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.34901962, 0.9843138 , 0.9450981 ,0.3372549 , 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.01960784,0.8078432 , 0.96470594, 0.6156863 , 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.01568628, 0.45882356, 0.27058825,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.], dtype=np.float32)\n",
        "image_torch = torch.from_numpy(image).view(1, 1, 28, 28)\n",
        "print(image_torch)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.3804, 0.3765, 0.3020, 0.4627, 0.2392,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.3529, 0.5412, 0.9216, 0.9216, 0.9216,\n",
            "           0.9216, 0.9216, 0.9216, 0.9843, 0.9843, 0.9725, 0.9961, 0.9608,\n",
            "           0.9216, 0.7451, 0.0824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.5490, 0.9843, 0.9961, 0.9961, 0.9961, 0.9961,\n",
            "           0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
            "           0.9961, 0.9961, 0.7412, 0.0902, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.8863, 0.9961, 0.8157, 0.7804, 0.7804, 0.7804,\n",
            "           0.7804, 0.5451, 0.2392, 0.2392, 0.2392, 0.2392, 0.2392, 0.5020,\n",
            "           0.8706, 0.9961, 0.9961, 0.7412, 0.0824, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.1490, 0.3216, 0.0510, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.1333, 0.8353, 0.9961, 0.9961, 0.4510, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.3294, 0.9961, 0.9961, 0.9176, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.3294, 0.9961, 0.9961, 0.9176, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.4157, 0.6157, 0.9961, 0.9961, 0.9529, 0.2000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0980, 0.4588, 0.8941, 0.8941, 0.8941,\n",
            "           0.9922, 0.9961, 0.9961, 0.9961, 0.9961, 0.9412, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.2667, 0.4667, 0.8627, 0.9961, 0.9961, 0.9961, 0.9961,\n",
            "           0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.5569, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1451,\n",
            "           0.7333, 0.9922, 0.9961, 0.9961, 0.9961, 0.8745, 0.8078, 0.8078,\n",
            "           0.2941, 0.2667, 0.8431, 0.9961, 0.9961, 0.4588, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4431, 0.8588,\n",
            "           0.9961, 0.9490, 0.8902, 0.4510, 0.3490, 0.1216, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.7843, 0.9961, 0.9451, 0.1608, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6627, 0.9961,\n",
            "           0.6902, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.1882, 0.9059, 0.9961, 0.9176, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.4863,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.3294, 0.9961, 0.9961, 0.6510, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.5451, 0.9961, 0.9333, 0.2235, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.8235, 0.9804, 0.9961, 0.6588, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.9490, 0.9961, 0.9373, 0.2235, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3490,\n",
            "           0.9843, 0.9451, 0.3373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.8078,\n",
            "           0.9647, 0.6157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.4588,\n",
            "           0.2706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28-oVco6MTib",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "d1578ac1-4aa2-4321-946b-ef7f5bb187f5"
      },
      "source": [
        "# a gaussian blur kernel\n",
        "gaussian_kernel = torch.tensor([[1., 2, 1],[2, 4, 2],[1, 2, 1]]) / 16.0\n",
        "\n",
        "# 这里怎么少一个dimension？不是N，channel，height 和width吗\n",
        "# 不，是input channel，output channel，和kernel size，如果是正方形的话，kernel size是一个数字就行了，不是的话得来个括号，然后里面是（长，宽）\n",
        "\n",
        "# 这两个都一样\n",
        "conv = nn.Conv2d(1, 1, (3,3))\n",
        "conv = nn.Conv2d(1, 1, 3)\n",
        "\n",
        "# conv = nn.Conv2d(1, 1, 3, padding=1)\n",
        "\n",
        "\n",
        "print(conv)\n",
        "# manually set the conv weight\n",
        "print(conv.weight.data)\n",
        "# conv.weight.data[:] = gaussian_kernel\n",
        "\n",
        "convolved = conv(image_torch)\n",
        "\n",
        "plt.title('original image')\n",
        "plt.imshow(image_torch.view(28,28).detach().numpy())\n",
        "plt.show()\n",
        "\n",
        "plt.title('blurred image')\n",
        "# 这里变成了26x26是因为没有padding\n",
        "plt.imshow(convolved.view(26,26).detach().numpy())\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "tensor([[[[ 0.2700, -0.1466, -0.0295],\n",
            "          [ 0.3326, -0.2399,  0.2092],\n",
            "          [-0.2688, -0.0176,  0.0421]]]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhf848TRMTid"
      },
      "source": [
        "As we can see, the image is blurred as expected. \n",
        "\n",
        "In practice, we learn many kernels at a time. In this example, we take in an RGB image (3 channels) and output a 16 channel image. After an activation function, that could be used as input to another `Conv2d` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noG9FyJ0MTie",
        "outputId": "af09dd44-d6ed-425e-d221-49e1a1e920d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "im_channels = 3 # if we are working with RGB images, there are 3 input channels, with black and white, 1\n",
        "out_channels = 16 # this is a hyperparameter we can tune\n",
        "kernel_size = 3 # this is another hyperparameter we can tune\n",
        "batch_size = 4\n",
        "image_width = 32\n",
        "image_height = 32\n",
        "\n",
        "im = torch.randn(batch_size, im_channels, image_width, image_height)\n",
        "\n",
        "m = nn.Conv2d(im_channels, out_channels, kernel_size)\n",
        "convolved = m(im) # it is a module so we can call it\n",
        "\n",
        "print('im shape', im.shape)\n",
        "print('convolved im shape', convolved.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "im shape torch.Size([4, 3, 32, 32])\n",
            "convolved im shape torch.Size([4, 16, 30, 30])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwsmNTYLMTig"
      },
      "source": [
        "## Useful links:\n",
        "- [60 minute PyTorch Tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "- [PyTorch Docs](https://pytorch.org/docs/stable/index.html)\n",
        "- [Lecture notes on Auto-Diff](https://courses.cs.washington.edu/courses/cse446/19wi/notes/auto-diff.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d77LgKaMTih"
      },
      "source": [
        "\n",
        "Custom Datasets, DataLoaders\n",
        "===================================================\n",
        "This is modified from pytorch official tutorial.\n",
        "**Author**: `Sasank Chilamkurthy <https://chsasank.github.io>`_\n",
        "\n",
        "A lot of effort in solving any machine learning problem goes in to\n",
        "preparing the data. PyTorch provides many tools to make data loading\n",
        "easy and hopefully, to make your code more readable. In this tutorial,\n",
        "we will see how to load and preprocess/augment data from a non trivial\n",
        "dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyN-mHRoMTii"
      },
      "source": [
        "Dataset class\n",
        "-------------\n",
        "\n",
        "``torch.utils.data.Dataset`` is an abstract class representing a\n",
        "dataset.\n",
        "Your custom dataset should inherit ``Dataset`` and override the following\n",
        "methods:\n",
        "\n",
        "-  ``__len__`` so that ``len(dataset)`` returns the size of the dataset.\n",
        "-  ``__getitem__`` to support the indexing such that ``dataset[i]`` can\n",
        "   be used to get $i$\\ th sample\n",
        "\n",
        "Let's create a dataset class for our face landmarks dataset. We will\n",
        "read the csv in ``__init__`` but leave the reading of images to\n",
        "``__getitem__``. This is memory efficient because all the images are not\n",
        "stored in the memory at once but read as required.\n",
        "\n",
        "Sample of our dataset will be a dict\n",
        "``{'image': image, 'landmarks': landmarks}``. Our dataset will take an\n",
        "optional argument ``transform`` so that any required processing can be\n",
        "applied on the sample. We will see the usefulness of ``transform`` in the\n",
        "next section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I302HaeiMTij"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class FakeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "# 必须得有__getitem__, 否则下一步用DataLoader时会不知道怎么分割batch\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOgpiQcIMTik"
      },
      "source": [
        "However, we are losing a lot of features by using a simple ``for`` loop to\n",
        "iterate over the data. In particular, we are missing out on:\n",
        "\n",
        "-  Batching the data\n",
        "-  Shuffling the data\n",
        "-  Load the data in parallel using ``multiprocessing`` workers.\n",
        "\n",
        "``torch.utils.data.DataLoader`` is an iterator which provides all these\n",
        "features. Parameters used below should be clear. One parameter of\n",
        "interest is ``collate_fn``. You can specify how exactly the samples need\n",
        "to be batched using ``collate_fn``. However, default collate should work\n",
        "fine for most use cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMHL8d06MTik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05326ed2-1faa-4e37-ab5e-21b56f5d758d"
      },
      "source": [
        "x = np.random.rand(100, 10)\n",
        "y = np.random.rand(100)\n",
        "\n",
        "dataset = FakeDataset(x, y)\n",
        "dataloader = DataLoader(dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=2)\n",
        "\n",
        "for i_batch, sample_batched in enumerate(dataloader):\n",
        "    print(i_batch, sample_batched)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [tensor([[0.7193, 0.2741, 0.4420, 0.1557, 0.9420, 0.5117, 0.5931, 0.8154, 0.5361,\n",
            "         0.1872],\n",
            "        [0.5676, 0.6626, 0.4767, 0.1390, 0.2816, 0.0409, 0.5260, 0.3794, 0.2643,\n",
            "         0.3537],\n",
            "        [0.9096, 0.0918, 0.0696, 0.2971, 0.7389, 0.8383, 0.3929, 0.0389, 0.9189,\n",
            "         0.6199],\n",
            "        [0.2380, 0.1677, 0.9380, 0.1448, 0.6922, 0.6069, 0.8344, 0.5863, 0.1457,\n",
            "         0.2014]], dtype=torch.float64), tensor([0.2176, 0.9754, 0.8536, 0.3532], dtype=torch.float64)]\n",
            "1 [tensor([[0.0314, 0.0474, 0.9028, 0.9867, 0.0420, 0.1916, 0.1678, 0.7979, 0.1753,\n",
            "         0.9613],\n",
            "        [0.5514, 0.5059, 0.9791, 0.1536, 0.3263, 0.5719, 0.7523, 0.8745, 0.0716,\n",
            "         0.7475],\n",
            "        [0.5241, 0.7461, 0.4563, 0.6050, 0.6101, 0.5824, 0.1442, 0.3367, 0.7613,\n",
            "         0.4690],\n",
            "        [0.1643, 0.4962, 0.2381, 0.5720, 0.7894, 0.4712, 0.7996, 0.9251, 0.0110,\n",
            "         0.3813]], dtype=torch.float64), tensor([0.7319, 0.4930, 0.8746, 0.4445], dtype=torch.float64)]\n",
            "2 [tensor([[0.5844, 0.8862, 0.0699, 0.6444, 0.5367, 0.9208, 0.7480, 0.0144, 0.9199,\n",
            "         0.0995],\n",
            "        [0.2806, 0.6659, 0.6772, 0.3973, 0.5594, 0.8148, 0.9992, 0.5291, 0.9006,\n",
            "         0.5723],\n",
            "        [0.5862, 0.8508, 0.7199, 0.1911, 0.5497, 0.0289, 0.6275, 0.8197, 0.8303,\n",
            "         0.1428],\n",
            "        [0.1158, 0.9346, 0.5718, 0.8236, 0.0688, 0.4658, 0.1309, 0.2279, 0.5726,\n",
            "         0.6154]], dtype=torch.float64), tensor([0.4240, 0.4308, 0.4070, 0.8229], dtype=torch.float64)]\n",
            "3 [tensor([[0.2483, 0.8856, 0.3049, 0.1242, 0.7612, 0.2720, 0.4562, 0.6301, 0.3054,\n",
            "         0.7610],\n",
            "        [0.7922, 0.8721, 0.7375, 0.4412, 0.8387, 0.6168, 0.0745, 0.5748, 0.8836,\n",
            "         0.8897],\n",
            "        [0.2011, 0.2319, 0.7576, 0.3040, 0.3492, 0.1767, 0.7191, 0.6431, 0.7997,\n",
            "         0.4290],\n",
            "        [0.8193, 0.5891, 0.7714, 0.1626, 0.7857, 0.8589, 0.3915, 0.9859, 0.1276,\n",
            "         0.9227]], dtype=torch.float64), tensor([0.3197, 0.5162, 0.1961, 0.3965], dtype=torch.float64)]\n",
            "4 [tensor([[0.7286, 0.3560, 0.7249, 0.2052, 0.7728, 0.2892, 0.8883, 0.8719, 0.9782,\n",
            "         0.7011],\n",
            "        [0.5232, 0.9802, 0.1296, 0.1223, 0.8721, 0.9366, 0.4230, 0.1198, 0.5949,\n",
            "         0.4491],\n",
            "        [0.8724, 0.5149, 0.3893, 0.9610, 0.5823, 0.1067, 0.2902, 0.5042, 0.6721,\n",
            "         0.1359],\n",
            "        [0.8782, 0.1287, 0.7829, 0.1273, 0.3311, 0.1643, 0.0239, 0.1828, 0.8095,\n",
            "         0.5271]], dtype=torch.float64), tensor([0.9256, 0.5575, 0.7144, 0.4192], dtype=torch.float64)]\n",
            "5 [tensor([[0.8131, 0.2481, 0.9994, 0.9208, 0.1911, 0.9005, 0.4690, 0.2828, 0.7544,\n",
            "         0.4733],\n",
            "        [0.3972, 0.9308, 0.9525, 0.5032, 0.2424, 0.4351, 0.8631, 0.3911, 0.3436,\n",
            "         0.3880],\n",
            "        [0.5676, 0.7747, 0.0766, 0.8108, 0.0686, 0.8337, 0.6514, 0.2370, 0.3409,\n",
            "         0.0724],\n",
            "        [0.7599, 0.3493, 0.5720, 0.5422, 0.9541, 0.3611, 0.6543, 0.5687, 0.9864,\n",
            "         0.5154]], dtype=torch.float64), tensor([0.9470, 0.1105, 0.0977, 0.5998], dtype=torch.float64)]\n",
            "6 [tensor([[0.8666, 0.4252, 0.7587, 0.6702, 0.2597, 0.6034, 0.3960, 0.8095, 0.9669,\n",
            "         0.5710],\n",
            "        [0.0103, 0.6414, 0.6822, 0.2235, 0.3180, 0.9841, 0.1115, 0.1755, 0.8579,\n",
            "         0.1777],\n",
            "        [0.9593, 0.8794, 0.2167, 0.6176, 0.2897, 0.0233, 0.4649, 0.4352, 0.4179,\n",
            "         0.1543],\n",
            "        [0.1820, 0.2417, 0.8614, 0.2018, 0.2046, 0.6464, 0.9676, 0.7790, 0.2166,\n",
            "         0.5023]], dtype=torch.float64), tensor([0.1514, 0.5299, 0.3174, 0.5208], dtype=torch.float64)]\n",
            "7 [tensor([[0.2470, 0.9688, 0.4355, 0.3492, 0.8750, 0.3249, 0.5220, 0.4653, 0.3597,\n",
            "         0.1476],\n",
            "        [0.7581, 0.3579, 0.4255, 0.1752, 0.2802, 0.3451, 0.0837, 0.5392, 0.9778,\n",
            "         0.3526],\n",
            "        [0.3092, 0.2737, 0.4736, 0.8256, 0.9713, 0.4716, 0.6563, 0.6695, 0.2421,\n",
            "         0.6513],\n",
            "        [0.0952, 0.8205, 0.5406, 0.6423, 0.8216, 0.1433, 0.4484, 0.2387, 0.8781,\n",
            "         0.0686]], dtype=torch.float64), tensor([0.9187, 0.7418, 0.1066, 0.9498], dtype=torch.float64)]\n",
            "8 [tensor([[0.7813, 0.6415, 0.3381, 0.4081, 0.2997, 0.6885, 0.5729, 0.9190, 0.6803,\n",
            "         0.6488],\n",
            "        [0.2712, 0.5985, 0.6606, 0.2495, 0.5750, 0.3125, 0.2018, 0.2139, 0.9962,\n",
            "         0.6229],\n",
            "        [0.0305, 0.5266, 0.8390, 0.1262, 0.0888, 0.3125, 0.2020, 0.4675, 0.0024,\n",
            "         0.0545],\n",
            "        [0.4927, 0.5544, 0.7241, 0.5206, 0.6984, 0.8743, 0.4420, 0.3300, 0.0746,\n",
            "         0.1899]], dtype=torch.float64), tensor([0.5924, 0.3642, 0.4210, 0.9091], dtype=torch.float64)]\n",
            "9 [tensor([[0.0017, 0.8764, 0.4341, 0.4187, 0.1138, 0.5352, 0.1087, 0.8659, 0.8653,\n",
            "         0.9963],\n",
            "        [0.6259, 0.7619, 0.3460, 0.0873, 0.4160, 0.6447, 0.5378, 0.1851, 0.6067,\n",
            "         0.2988],\n",
            "        [0.4844, 0.6111, 0.9362, 0.9658, 0.0603, 0.6910, 0.3450, 0.8205, 0.4409,\n",
            "         0.9758],\n",
            "        [0.3612, 0.4280, 0.4271, 0.3757, 0.2818, 0.1289, 0.7074, 0.2780, 0.0569,\n",
            "         0.5659]], dtype=torch.float64), tensor([0.7910, 0.1803, 0.1892, 0.4506], dtype=torch.float64)]\n",
            "10 [tensor([[0.8477, 0.1110, 0.7232, 0.5329, 0.0023, 0.5219, 0.4534, 0.1643, 0.0425,\n",
            "         0.9919],\n",
            "        [0.9997, 0.6225, 0.5405, 0.5993, 0.9458, 0.9620, 0.7767, 0.8790, 0.5936,\n",
            "         0.4575],\n",
            "        [0.3776, 0.7277, 0.1168, 0.6482, 0.5110, 0.6284, 0.4759, 0.1284, 0.5939,\n",
            "         0.3573],\n",
            "        [0.1343, 0.2781, 0.3265, 0.5423, 0.9113, 0.6730, 0.1282, 0.5794, 0.4786,\n",
            "         0.3136]], dtype=torch.float64), tensor([0.0529, 0.4465, 0.0415, 0.8234], dtype=torch.float64)]\n",
            "11 [tensor([[0.4198, 0.0709, 0.9849, 0.2112, 0.3824, 0.0547, 0.0032, 0.2437, 0.1643,\n",
            "         0.2527],\n",
            "        [0.5271, 0.7308, 0.2606, 0.3060, 0.4713, 0.8939, 0.2696, 0.6694, 0.7174,\n",
            "         0.7690],\n",
            "        [0.9640, 0.3807, 0.9718, 0.5965, 0.9739, 0.8423, 0.9324, 0.8974, 0.9478,\n",
            "         0.8207],\n",
            "        [0.0524, 0.2018, 0.9729, 0.9746, 0.7646, 0.3512, 0.5527, 0.7193, 0.1196,\n",
            "         0.9968]], dtype=torch.float64), tensor([0.4475, 0.5668, 0.6453, 0.1496], dtype=torch.float64)]\n",
            "12 [tensor([[0.8105, 0.7770, 0.7919, 0.2277, 0.4961, 0.4951, 0.8445, 0.8258, 0.2101,\n",
            "         0.1465],\n",
            "        [0.8091, 0.6970, 0.2666, 0.9707, 0.0376, 0.6283, 0.5412, 0.1177, 0.3108,\n",
            "         0.7837],\n",
            "        [0.6270, 0.7303, 0.3776, 0.3713, 0.0260, 0.8236, 0.8103, 0.7517, 0.6437,\n",
            "         0.3878],\n",
            "        [0.0843, 0.3827, 0.6860, 0.8026, 0.8955, 0.1372, 0.6357, 0.1304, 0.7118,\n",
            "         0.0218]], dtype=torch.float64), tensor([0.5114, 0.9116, 0.1550, 0.0615], dtype=torch.float64)]\n",
            "13 [tensor([[2.6391e-01, 9.6418e-02, 5.4907e-01, 8.3312e-01, 4.7135e-01, 1.2240e-01,\n",
            "         5.7202e-01, 8.2525e-01, 1.5255e-01, 3.4464e-02],\n",
            "        [5.6448e-01, 4.0810e-01, 7.2110e-01, 7.2452e-01, 9.4733e-01, 1.8188e-01,\n",
            "         3.5771e-01, 7.0004e-02, 7.5087e-01, 9.1636e-04],\n",
            "        [7.7344e-01, 4.5364e-01, 8.5756e-01, 9.6728e-01, 1.3364e-01, 1.5196e-01,\n",
            "         7.6917e-01, 8.4401e-01, 3.7903e-01, 3.4297e-01],\n",
            "        [4.7300e-02, 3.0849e-01, 2.1834e-01, 2.5558e-01, 6.0843e-01, 8.7484e-01,\n",
            "         6.5641e-01, 9.7748e-01, 2.2477e-01, 6.7694e-01]], dtype=torch.float64), tensor([0.2160, 0.9085, 0.6493, 0.0541], dtype=torch.float64)]\n",
            "14 [tensor([[0.7581, 0.2951, 0.7337, 0.3272, 0.3305, 0.6940, 0.7216, 0.8421, 0.7748,\n",
            "         0.2389],\n",
            "        [0.6346, 0.7938, 0.5907, 0.7033, 0.8878, 0.5589, 0.4709, 0.9064, 0.6376,\n",
            "         0.3697],\n",
            "        [0.5473, 0.1063, 0.9086, 0.1076, 0.2286, 0.3228, 0.6951, 0.7462, 0.7201,\n",
            "         0.5295],\n",
            "        [0.3169, 0.9958, 0.4501, 0.6739, 0.5561, 0.6017, 0.0974, 0.1254, 0.8591,\n",
            "         0.4724]], dtype=torch.float64), tensor([0.6282, 0.6679, 0.8601, 0.9078], dtype=torch.float64)]\n",
            "15 [tensor([[0.3366, 0.8714, 0.6115, 0.6810, 0.0068, 0.4266, 0.3561, 0.8192, 0.2736,\n",
            "         0.1911],\n",
            "        [0.9397, 0.3359, 0.9771, 0.4257, 0.3812, 0.3332, 0.9095, 0.0495, 0.1773,\n",
            "         0.7450],\n",
            "        [0.9341, 0.8192, 0.4794, 0.1369, 0.1570, 0.6007, 0.4061, 0.3166, 0.0013,\n",
            "         0.8705],\n",
            "        [0.2496, 0.5103, 0.7602, 0.7545, 0.7864, 0.6920, 0.0506, 0.3063, 0.7926,\n",
            "         0.0234]], dtype=torch.float64), tensor([0.0078, 0.8900, 0.3551, 0.5819], dtype=torch.float64)]\n",
            "16 [tensor([[0.8867, 0.4919, 0.7643, 0.7295, 0.3177, 0.8836, 0.2750, 0.3584, 0.3854,\n",
            "         0.8509],\n",
            "        [0.8784, 0.2035, 0.3684, 0.7283, 0.1674, 0.1531, 0.3919, 0.2363, 0.7387,\n",
            "         0.3775],\n",
            "        [0.5330, 0.0950, 0.9855, 0.3621, 0.1388, 0.6975, 0.0636, 0.5173, 0.6741,\n",
            "         0.8240],\n",
            "        [0.0810, 0.6199, 0.9772, 0.3341, 0.7157, 0.7078, 0.5406, 0.6877, 0.9456,\n",
            "         0.8123]], dtype=torch.float64), tensor([0.0541, 0.0730, 0.9929, 0.3848], dtype=torch.float64)]\n",
            "17 [tensor([[0.4461, 0.6816, 0.0353, 0.1587, 0.4847, 0.1587, 0.1219, 0.4906, 0.2270,\n",
            "         0.3038],\n",
            "        [0.1290, 0.8417, 0.0805, 0.7632, 0.6384, 0.4738, 0.2833, 0.4883, 0.4398,\n",
            "         0.2118],\n",
            "        [0.2940, 0.5913, 0.7963, 0.7031, 0.5146, 0.2531, 0.6995, 0.6184, 0.6100,\n",
            "         0.8252],\n",
            "        [0.7252, 0.1700, 0.4736, 0.5389, 0.5983, 0.1519, 0.1145, 0.5128, 0.4518,\n",
            "         0.8404]], dtype=torch.float64), tensor([0.6640, 0.5692, 0.5294, 0.6043], dtype=torch.float64)]\n",
            "18 [tensor([[0.6924, 0.9485, 0.0995, 0.0456, 0.6549, 0.9433, 0.6102, 0.5603, 0.2408,\n",
            "         0.9095],\n",
            "        [0.4829, 0.8438, 0.5721, 0.7199, 0.2099, 0.9154, 0.4669, 0.9186, 0.5964,\n",
            "         0.1298],\n",
            "        [0.1334, 0.1654, 0.7649, 0.2632, 0.1446, 0.8478, 0.8986, 0.2307, 0.6011,\n",
            "         0.0798],\n",
            "        [0.4193, 0.7755, 0.8711, 0.4332, 0.1093, 0.1503, 0.2798, 0.5293, 0.5112,\n",
            "         0.1526]], dtype=torch.float64), tensor([0.9190, 0.8809, 0.5637, 0.0659], dtype=torch.float64)]\n",
            "19 [tensor([[0.3235, 0.2608, 0.7946, 0.8532, 0.2414, 0.7997, 0.4260, 0.3402, 0.7103,\n",
            "         0.2384],\n",
            "        [0.9319, 0.0412, 0.0863, 0.9706, 0.2272, 0.6107, 0.4235, 0.0705, 0.5725,\n",
            "         0.4531],\n",
            "        [0.2352, 0.6853, 0.2502, 0.9014, 0.4604, 0.5132, 0.5149, 0.4850, 0.8034,\n",
            "         0.0400],\n",
            "        [0.5437, 0.9641, 0.9197, 0.6536, 0.8555, 0.3832, 0.4749, 0.1247, 0.5275,\n",
            "         0.9601]], dtype=torch.float64), tensor([0.2128, 0.2169, 0.0985, 0.6704], dtype=torch.float64)]\n",
            "20 [tensor([[0.0448, 0.0566, 0.1424, 0.3803, 0.8784, 0.8085, 0.0140, 0.6331, 0.3424,\n",
            "         0.6040],\n",
            "        [0.2304, 0.7732, 0.9229, 0.3098, 0.2822, 0.2590, 0.4790, 0.5289, 0.0908,\n",
            "         0.1693],\n",
            "        [0.3365, 0.0709, 0.2704, 0.3270, 0.8304, 0.4986, 0.0080, 0.7490, 0.0788,\n",
            "         0.4996],\n",
            "        [0.2386, 0.2582, 0.0272, 0.7206, 0.0868, 0.1632, 0.3009, 0.2066, 0.9160,\n",
            "         0.5051]], dtype=torch.float64), tensor([3.3145e-04, 1.8411e-01, 5.8064e-01, 3.7615e-01], dtype=torch.float64)]\n",
            "21 [tensor([[0.5824, 0.9879, 0.4630, 0.7460, 0.0692, 0.8884, 0.4320, 0.3974, 0.7387,\n",
            "         0.5217],\n",
            "        [0.4995, 0.1441, 0.4368, 0.0184, 0.7353, 0.0847, 0.4594, 0.2181, 0.4138,\n",
            "         0.0778],\n",
            "        [0.6212, 0.7634, 0.1739, 0.1661, 0.0909, 0.2586, 0.0904, 0.9020, 0.6789,\n",
            "         0.7973],\n",
            "        [0.6777, 0.0363, 0.4096, 0.9201, 0.0534, 0.7125, 0.8117, 0.7707, 0.2715,\n",
            "         0.1595]], dtype=torch.float64), tensor([0.7860, 0.0616, 0.0667, 0.1846], dtype=torch.float64)]\n",
            "22 [tensor([[0.4239, 0.5876, 0.9329, 0.2905, 0.9048, 0.7313, 0.4298, 0.5547, 0.6011,\n",
            "         0.9321],\n",
            "        [0.0340, 0.4668, 0.0750, 0.6089, 0.2053, 0.8292, 0.8497, 0.0069, 0.9975,\n",
            "         0.1573],\n",
            "        [0.4577, 0.1164, 0.8521, 0.5436, 0.1230, 0.8637, 0.6344, 0.3891, 0.9627,\n",
            "         0.3131],\n",
            "        [0.1722, 0.6639, 0.3853, 0.7410, 0.8874, 0.3659, 0.4992, 0.5379, 0.1294,\n",
            "         0.3643]], dtype=torch.float64), tensor([0.4682, 0.2120, 0.1009, 0.7623], dtype=torch.float64)]\n",
            "23 [tensor([[0.3284, 0.6882, 0.4232, 0.9338, 0.3786, 0.2986, 0.2512, 0.3313, 0.7084,\n",
            "         0.9636],\n",
            "        [0.9388, 0.7696, 0.9103, 0.1109, 0.6574, 0.5505, 0.9590, 0.1639, 0.5555,\n",
            "         0.3267],\n",
            "        [0.0771, 0.8194, 0.2498, 0.6350, 0.7631, 0.1724, 0.9001, 0.3070, 0.7412,\n",
            "         0.6617],\n",
            "        [0.4249, 0.5933, 0.4940, 0.7780, 0.8217, 0.7461, 0.6196, 0.7231, 0.9190,\n",
            "         0.7329]], dtype=torch.float64), tensor([0.8468, 0.0537, 0.2348, 0.9653], dtype=torch.float64)]\n",
            "24 [tensor([[0.3573, 0.8464, 0.8960, 0.2023, 0.9011, 0.7337, 0.8791, 0.8478, 0.0427,\n",
            "         0.0810],\n",
            "        [0.0968, 0.6311, 0.4649, 0.9188, 0.0242, 0.4175, 0.3894, 0.2893, 0.2589,\n",
            "         0.6001],\n",
            "        [0.9460, 0.1589, 0.9567, 0.2504, 0.5828, 0.9493, 0.4935, 0.9578, 0.1922,\n",
            "         0.1578],\n",
            "        [0.9475, 0.3444, 0.3755, 0.3436, 0.3452, 0.6241, 0.9106, 0.0189, 0.4924,\n",
            "         0.7676]], dtype=torch.float64), tensor([0.1340, 0.2551, 0.9985, 0.3006], dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m-ml0NoMTim"
      },
      "source": [
        "Mixed Presision Training\n",
        "===================================================\n",
        "**Author**: `Chi-Liang Liu <https://liangtaiwan.github.io>`\n",
        "**Ref**: https://github.com/NVIDIA/apex\n",
        "Using mixed precision to train your networks can be:\n",
        "- 2-4x faster\n",
        "- memory-efficient\n",
        "in only 3 lines of Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a94qxAC_MTin"
      },
      "source": [
        "# Apex \n",
        "\n",
        "NVIDIA-maintained utilities to streamline mixed precision and distributed training in Pytorch. Some of the code here will be included in upstream Pytorch eventually. The intention of Apex is to make up-to-date utilities available to users as quickly as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw6ociH0MTin"
      },
      "source": [
        "## apex.amp\n",
        "\n",
        "Amp allows users to easily experiment with different pure and mixed precision modes.\n",
        "Commonly-used default modes are chosen by\n",
        "selecting an \"optimization level\" or ``opt_level``; each ``opt_level`` establishes a set of\n",
        "properties that govern Amp's implementation of pure or mixed precision training.\n",
        "Finer-grained control of how a given ``opt_level`` behaves can be achieved by passing values for\n",
        "particular properties directly to ``amp.initialize``.  These manually specified values\n",
        "override the defaults established by the ``opt_level``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxP-tvHcMTio"
      },
      "source": [
        "# 不懂什么是mixed precision training\n",
        "\n",
        "from apex import amp\n",
        "\n",
        "# Declare model and optimizer as usual, with default (FP32) precision\n",
        "model = torch.nn.Linear(10, 100).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Allow Amp to perform casts as required by the opt_level\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
        "...\n",
        "# loss.backward() becomes:\n",
        "with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "    scaled_loss.backward()\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}